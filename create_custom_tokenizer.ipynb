{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qdBenTpEKSYf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/mushfiq/Desktop/llm_fundamentals/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Fetch the token securely from Colab secrets\n",
        "hf_token = os.getenv(\"HF_TOKEN\")\n",
        "\n",
        "if hf_token is None:\n",
        "    raise ValueError(\"HF_TOKEN not found. Please add it via Colab secrets.\")\n",
        "\n",
        "login(token=hf_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OG_TDiFzRzxE"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import unicodedata\n",
        "\n",
        "def normalize_text(text: str) -> str:\n",
        "    text = unicodedata.normalize(\"NFKC\", text)                # Normalize Unicode\n",
        "    text = text.replace(\"’\", \"'\")                             # Fix smart apostrophe\n",
        "    text = text.replace(\"“\", '\"').replace(\"”\", '\"')           # Fix smart quotes\n",
        "    text = text.replace(\"–\", \"-\").replace(\"—\", \"-\")           # Fix dashes\n",
        "    text = re.sub(r\"[^\\x00-\\x7F]+\", \" \", text)                # Remove any remaining non-ASCII\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()                  # Collapse whitespace\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhUWzxqCLHgJ",
        "outputId": "3b3cbb42-d755-48a9-e536-524a451ecb98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Loaded 5572 samples\n",
            "Example: Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def load_corpus(file_path: str) -> list:\n",
        "    df = pd.read_csv(file_path, encoding=\"latin-1\")\n",
        "    text_data = df[\"v2\"].dropna().astype(str).tolist()\n",
        "    text_data = [normalize_text(t) for t in text_data]\n",
        "    return text_data\n",
        "\n",
        "corpus = load_corpus(\"data/spam.csv\")\n",
        "print(f\"✅ Loaded {len(corpus)} samples\")\n",
        "print(f\"Example: {corpus[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "16X50gIJLyfd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from tokenizers import ByteLevelBPETokenizer\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace, ByteLevel\n",
        "import os\n",
        "\n",
        "def train_tokenizer(corpus: list, output_dir: str, vocab_size: int = 10000) -> ByteLevelBPETokenizer:\n",
        "    \"\"\"\n",
        "    Trains a Byte-Level BPE tokenizer on a given corpus.\n",
        "\n",
        "    Args:\n",
        "        corpus (list): List of text strings.\n",
        "        output_dir (str): Path to save tokenizer files.\n",
        "        vocab_size (int): Vocabulary size to train on.\n",
        "\n",
        "    Returns:\n",
        "        ByteLevelBPETokenizer: Trained tokenizer.\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    tokenizer = ByteLevelBPETokenizer()\n",
        "    tokenizer._tokenizer.pre_tokenizer = ByteLevel()\n",
        "\n",
        "\n",
        "    tokenizer.train_from_iterator(\n",
        "        corpus,\n",
        "        vocab_size=vocab_size,\n",
        "        min_frequency=2,\n",
        "        show_progress=True,\n",
        "        special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"]\n",
        "    )\n",
        "    tokenizer.save_model(output_dir)\n",
        "    tokenizer.save(f\"{output_dir}/tokenizer.json\")  # ✅ Required for Hugging Face compatibility\n",
        "    return tokenizer\n",
        "\n",
        "tokenizer = train_tokenizer(corpus, output_dir=\"my_tokenizer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ae79EKliNcrp"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
          ]
        }
      ],
      "source": [
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "def load_fast_tokenizer(tokenizer_dir: str) -> PreTrainedTokenizerFast:\n",
        "    \"\"\"\n",
        "    Loads tokenizer files into a PreTrainedTokenizerFast object.\n",
        "\n",
        "    Args:\n",
        "        tokenizer_dir (str): Directory where tokenizer files are saved.\n",
        "\n",
        "    Returns:\n",
        "        PreTrainedTokenizerFast: Hugging Face compatible tokenizer.\n",
        "    \"\"\"\n",
        "    return PreTrainedTokenizerFast(\n",
        "        tokenizer_file=f\"{tokenizer_dir}/tokenizer.json\",\n",
        "        unk_token=\"<unk>\",\n",
        "        pad_token=\"<pad>\",\n",
        "        bos_token=\"<s>\",\n",
        "        eos_token=\"</s>\",\n",
        "        mask_token=\"<mask>\"\n",
        "    )\n",
        "\n",
        "fast_tokenizer = load_fast_tokenizer(\"my_tokenizer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkE_97-KOfNu",
        "outputId": "ef66e480-6684-4797-bd18-d3b948c111e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Tokenizer pushed: https://huggingface.co/mushfiqurrobin/spam-tokenizer-bpe\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import HfApi\n",
        "\n",
        "HF_USERNAME = HfApi().whoami()[\"name\"]\n",
        "REPO_NAME = \"spam-tokenizer-bpe\"\n",
        "TOKENIZER_REPO = f\"{HF_USERNAME}/{REPO_NAME}\"\n",
        "\n",
        "fast_tokenizer.save_pretrained(\"my_tokenizer\")  # includes config files\n",
        "fast_tokenizer.push_to_hub(REPO_NAME)\n",
        "\n",
        "print(f\"✅ Tokenizer pushed: https://huggingface.co/{TOKENIZER_REPO}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEqfroiuPRJR",
        "outputId": "946fe34b-0a5a-40e7-ee6c-a0a7dfe1e705"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text: Congratulations! You’ve won a free ticket to Bahamas.\n",
            "Tokens: ['C', 'ongrat', 'ulations', '!', 'ĠYou', 'â', 'Ģ', 'Ļ', 've', 'Ġwon', 'Ġa', 'Ġfree', 'Ġticket', 'Ġto', 'ĠBahamas', '.']\n",
            "Token IDs: [39, 1322, 2254, 5, 410, 163, 227, 252, 294, 689, 262, 645, 4175, 276, 9676, 18]\n"
          ]
        }
      ],
      "source": [
        "text = \"Congratulations! You’ve won a free ticket to Bahamas.\"\n",
        "tokens = fast_tokenizer.tokenize(text)\n",
        "ids = fast_tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "print(\"Text:\", text)\n",
        "print(\"Tokens:\", tokens)\n",
        "print(\"Token IDs:\", ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAYtiOD0XqhE",
        "outputId": "a9fe015c-0545-4ef5-842e-088657ebadf1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokens: ['H', 'ello', '!', 'ĠThis', 'Ġis', 'Ġa', 'Ġdifferent', 'Ġsentence', 'Ġto', 'Ġtok', 'en', 'ize', '.']\n",
            "Token IDs: [44, 7284, 5, 839, 327, 262, 2933, 6447, 276, 5917, 307, 2300, 18]\n"
          ]
        }
      ],
      "source": [
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "# Load the tokenizer from the directory\n",
        "fast_tokenizer = PreTrainedTokenizerFast.from_pretrained(\"my_tokenizer\")\n",
        "\n",
        "# New text to tokenize\n",
        "text = \"Hello! This is a different sentence to tokenize.\"\n",
        "\n",
        "# Tokenize and get token IDs\n",
        "tokens = fast_tokenizer.tokenize(text)\n",
        "token_ids = fast_tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "print(\"Tokens:\", tokens)\n",
        "print(\"Token IDs:\", token_ids)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
