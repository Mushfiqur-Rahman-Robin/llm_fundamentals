{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qdBenTpEKSYf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/mushfiq/Desktop/llm_fundamentals/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Fetch the token securely from Colab secrets\n",
        "hf_token = os.getenv(\"HF_TOKEN\")\n",
        "\n",
        "if hf_token is None:\n",
        "    raise ValueError(\"HF_TOKEN not found. Please add it via Colab secrets.\")\n",
        "\n",
        "login(token=hf_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OG_TDiFzRzxE"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import unicodedata\n",
        "\n",
        "def normalize_text(text: str) -> str:\n",
        "    text = unicodedata.normalize(\"NFKC\", text)                # Normalize Unicode\n",
        "    text = text.replace(\"‚Äô\", \"'\")                             # Fix smart apostrophe\n",
        "    text = text.replace(\"‚Äú\", '\"').replace(\"‚Äù\", '\"')           # Fix smart quotes\n",
        "    text = text.replace(\"‚Äì\", \"-\").replace(\"‚Äî\", \"-\")           # Fix dashes\n",
        "    text = re.sub(r\"[^\\x00-\\x7F]+\", \" \", text)                # Remove any remaining non-ASCII\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()                  # Collapse whitespace\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhUWzxqCLHgJ",
        "outputId": "3b3cbb42-d755-48a9-e536-524a451ecb98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Loaded 5572 samples\n",
            "Example: Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def load_corpus(file_path: str) -> list:\n",
        "    df = pd.read_csv(file_path, encoding=\"latin-1\")\n",
        "    text_data = df[\"v2\"].dropna().astype(str).tolist()\n",
        "    text_data = [normalize_text(t) for t in text_data]\n",
        "    return text_data\n",
        "\n",
        "corpus = load_corpus(\"data/spam.csv\")\n",
        "print(f\"‚úÖ Loaded {len(corpus)} samples\")\n",
        "print(f\"Example: {corpus[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "16X50gIJLyfd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from tokenizers import ByteLevelBPETokenizer\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace, ByteLevel\n",
        "import os\n",
        "\n",
        "def train_tokenizer(corpus: list, output_dir: str, vocab_size: int = 10000) -> ByteLevelBPETokenizer:\n",
        "    \"\"\"\n",
        "    Trains a Byte-Level BPE tokenizer on a given corpus.\n",
        "\n",
        "    Args:\n",
        "        corpus (list): List of text strings.\n",
        "        output_dir (str): Path to save tokenizer files.\n",
        "        vocab_size (int): Vocabulary size to train on.\n",
        "\n",
        "    Returns:\n",
        "        ByteLevelBPETokenizer: Trained tokenizer.\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    tokenizer = ByteLevelBPETokenizer()\n",
        "    tokenizer._tokenizer.pre_tokenizer = ByteLevel()\n",
        "\n",
        "\n",
        "    tokenizer.train_from_iterator(\n",
        "        corpus,\n",
        "        vocab_size=vocab_size,\n",
        "        min_frequency=2,\n",
        "        show_progress=True,\n",
        "        special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"]\n",
        "    )\n",
        "    tokenizer.save_model(output_dir)\n",
        "    tokenizer.save(f\"{output_dir}/tokenizer.json\")  # ‚úÖ Required for Hugging Face compatibility\n",
        "    return tokenizer\n",
        "\n",
        "tokenizer = train_tokenizer(corpus, output_dir=\"my_tokenizer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "### üß† Step 1: Initialize the tokenizer\n",
        "\n",
        "```python\n",
        "tokenizer = ByteLevelBPETokenizer()\n",
        "tokenizer._tokenizer.pre_tokenizer = ByteLevel()\n",
        "```\n",
        "\n",
        "* `ByteLevelBPETokenizer()` initializes a tokenizer that:\n",
        "\n",
        "  * Operates at the byte level (good for handling rare or multilingual text).\n",
        "  * Uses Byte Pair Encoding (BPE) to learn subword units.\n",
        "\n",
        "* `tokenizer._tokenizer.pre_tokenizer = ByteLevel()` sets the **pre-tokenizer** to `ByteLevel`, which:\n",
        "\n",
        "  * Encodes the input text into bytes first.\n",
        "  * Handles spaces explicitly by marking them with a special character like `ƒ†` (important for preserving word boundaries).\n",
        "\n",
        "‚ö†Ô∏è Note: This low-level `. _tokenizer.pre_tokenizer` usage is necessary because `ByteLevelBPETokenizer` uses a legacy API; the higher-level abstraction is `tokenizer.pre_tokenizer`.\n",
        "\n",
        "---\n",
        "\n",
        "### üß™ Step 2: Train the tokenizer\n",
        "\n",
        "```python\n",
        "tokenizer.train_from_iterator(\n",
        "    corpus,\n",
        "    vocab_size=vocab_size,\n",
        "    min_frequency=2,\n",
        "    show_progress=True,\n",
        "    special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"]\n",
        ")\n",
        "```\n",
        "\n",
        "This tells the tokenizer to:\n",
        "\n",
        "* Train using the given `corpus` (an iterator of texts).\n",
        "* Limit the vocab size to `vocab_size`.\n",
        "* Ignore tokens that occur less than `min_frequency=2`.\n",
        "* Show progress during training.\n",
        "* Add 5 **special tokens** often used in NLP:\n",
        "\n",
        "  * `<s>` = start of sentence\n",
        "  * `</s>` = end of sentence\n",
        "  * `<pad>` = padding\n",
        "  * `<unk>` = unknown token\n",
        "  * `<mask>` = for masked language modeling\n",
        "\n",
        "---\n",
        "\n",
        "### üíæ Step 3: Save the tokenizer model\n",
        "\n",
        "```python\n",
        "tokenizer.save_model(output_dir)\n",
        "tokenizer.save(f\"{output_dir}/tokenizer.json\")\n",
        "```\n",
        "\n",
        "* `save_model()` saves the `vocab.json` and `merges.txt` files ‚Äî required for legacy loading.\n",
        "* `save(\"tokenizer.json\")` saves a **Hugging Face compatible** single-file JSON format, usable by `PreTrainedTokenizerFast`.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Final Output\n",
        "\n",
        "```python\n",
        "return tokenizer\n",
        "```\n",
        "\n",
        "Returns the trained `ByteLevelBPETokenizer` object so it can be reused if needed.\n",
        "\n",
        "---\n",
        "\n",
        "### üîÑ Summary of Output Files in `output_dir`\n",
        "\n",
        "| File             | Purpose                                |\n",
        "| ---------------- | -------------------------------------- |\n",
        "| `vocab.json`     | Maps tokens to IDs                     |\n",
        "| `merges.txt`     | Contains merge rules learned by BPE    |\n",
        "| `tokenizer.json` | Hugging Face-compatible tokenizer file |\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ae79EKliNcrp"
      },
      "outputs": [],
      "source": [
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "def load_fast_tokenizer(tokenizer_dir: str) -> PreTrainedTokenizerFast:\n",
        "    \"\"\"\n",
        "    Loads tokenizer files into a PreTrainedTokenizerFast object.\n",
        "\n",
        "    Args:\n",
        "        tokenizer_dir (str): Directory where tokenizer files are saved.\n",
        "\n",
        "    Returns:\n",
        "        PreTrainedTokenizerFast: Hugging Face compatible tokenizer.\n",
        "    \"\"\"\n",
        "    return PreTrainedTokenizerFast(\n",
        "        tokenizer_file=f\"{tokenizer_dir}/tokenizer.json\",\n",
        "        unk_token=\"<unk>\",\n",
        "        pad_token=\"<pad>\",\n",
        "        bos_token=\"<s>\",\n",
        "        eos_token=\"</s>\",\n",
        "        mask_token=\"<mask>\"\n",
        "    )\n",
        "\n",
        "fast_tokenizer = load_fast_tokenizer(\"my_tokenizer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `fast_tokenizer` here refers to a **Hugging Face-compatible tokenizer** that is created by wrapping the trained tokenizer files (`tokenizer.json`, `vocab.json`, `merges.txt`) using the class:\n",
        "\n",
        "```python\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üí° What is `PreTrainedTokenizerFast`?\n",
        "\n",
        "`PreTrainedTokenizerFast` is a **fast implementation of Hugging Face tokenizers** using the [ü§ó `tokenizers` Rust-based library](https://github.com/huggingface/tokenizers), which is:\n",
        "\n",
        "* ‚ö° **Much faster** than Python-based tokenizers.\n",
        "* ‚úÖ Fully compatible with `transformers` models.\n",
        "* üß† Capable of handling everything: tokenization, detokenization, ID mapping, padding, truncation, etc.\n",
        "* üß© Works with any tokenizer trained and saved as a `tokenizer.json`.\n",
        "\n",
        "---\n",
        "\n",
        "### üîÑ What the code does\n",
        "\n",
        "```python\n",
        "fast_tokenizer = load_fast_tokenizer(\"my_tokenizer\")\n",
        "```\n",
        "\n",
        "This loads the custom tokenizer you trained earlier with `ByteLevelBPETokenizer` and wraps it into a Hugging Face-compatible format.\n",
        "\n",
        "The important part is here:\n",
        "\n",
        "```python\n",
        "PreTrainedTokenizerFast(\n",
        "    tokenizer_file=f\"{tokenizer_dir}/tokenizer.json\",\n",
        "    unk_token=\"<unk>\",\n",
        "    pad_token=\"<pad>\",\n",
        "    bos_token=\"<s>\",\n",
        "    eos_token=\"</s>\",\n",
        "    mask_token=\"<mask>\"\n",
        ")\n",
        "```\n",
        "\n",
        "This:\n",
        "\n",
        "* Loads `tokenizer.json` (which contains vocab, merges, config).\n",
        "* Defines all special tokens to work seamlessly with Transformer models.\n",
        "\n",
        "Now, you can use `fast_tokenizer` just like any pretrained tokenizer (e.g., `BertTokenizerFast`, `GPT2TokenizerFast`, etc.):\n",
        "\n",
        "```python\n",
        "text = \"Hello world!\"\n",
        "tokens = fast_tokenizer.tokenize(text)\n",
        "ids = fast_tokenizer.convert_tokens_to_ids(tokens)\n",
        "decoded = fast_tokenizer.decode(ids)\n",
        "\n",
        "print(tokens)  # ['H', 'ello', 'ƒ†world', '!']\n",
        "print(ids)     # [id1, id2, id3, id4]\n",
        "print(decoded) # 'Hello world!'\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Summary\n",
        "\n",
        "| Concept                   | What it does                                                               |\n",
        "| ------------------------- | -------------------------------------------------------------------------- |\n",
        "| `PreTrainedTokenizerFast` | A wrapper to load and use a tokenizer trained with `tokenizers`            |\n",
        "| `tokenizer.json`          | The tokenizer model (vocab, merges, config)                                |\n",
        "| `fast_tokenizer`          | The fully functional tokenizer you can now use in your pipeline            |\n",
        "| Benefit                   | Fast, efficient, and works with Hugging Face `transformers` out-of-the-box |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkE_97-KOfNu",
        "outputId": "ef66e480-6684-4797-bd18-d3b948c111e2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No files have been modified since last commit. Skipping to prevent empty commit.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Tokenizer pushed: https://huggingface.co/mushfiqurrobin/spam-tokenizer-bpe\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import HfApi\n",
        "\n",
        "HF_USERNAME = HfApi().whoami()[\"name\"]\n",
        "REPO_NAME = \"spam-tokenizer-bpe\"\n",
        "TOKENIZER_REPO = f\"{HF_USERNAME}/{REPO_NAME}\"\n",
        "\n",
        "fast_tokenizer.save_pretrained(\"my_tokenizer\")  # includes config files\n",
        "fast_tokenizer.push_to_hub(REPO_NAME)\n",
        "\n",
        "print(f\"‚úÖ Tokenizer pushed: https://huggingface.co/{TOKENIZER_REPO}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEqfroiuPRJR",
        "outputId": "946fe34b-0a5a-40e7-ee6c-a0a7dfe1e705"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text: Congratulations! You‚Äôve won a free ticket to Bahamas.\n",
            "Tokens: ['C', 'ongrat', 'ulations', '!', 'ƒ†You', '√¢', 'ƒ¢', 'ƒª', 've', 'ƒ†won', 'ƒ†a', 'ƒ†free', 'ƒ†ticket', 'ƒ†to', 'ƒ†Bahamas', '.']\n",
            "Token IDs: [39, 1322, 2254, 5, 410, 163, 227, 252, 294, 689, 262, 645, 4175, 276, 9676, 18]\n"
          ]
        }
      ],
      "source": [
        "text = \"Congratulations! You‚Äôve won a free ticket to Bahamas.\"\n",
        "tokens = fast_tokenizer.tokenize(text)\n",
        "ids = fast_tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "print(\"Text:\", text)\n",
        "print(\"Tokens:\", tokens)\n",
        "print(\"Token IDs:\", ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAYtiOD0XqhE",
        "outputId": "a9fe015c-0545-4ef5-842e-088657ebadf1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokens: ['H', 'ello', '!', 'ƒ†This', 'ƒ†is', 'ƒ†a', 'ƒ†different', 'ƒ†sentence', 'ƒ†to', 'ƒ†tok', 'en', 'ize', '.']\n",
            "Token IDs: [44, 7284, 5, 839, 327, 262, 2933, 6447, 276, 5917, 307, 2300, 18]\n"
          ]
        }
      ],
      "source": [
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "# Load the tokenizer from the directory\n",
        "fast_tokenizer = PreTrainedTokenizerFast.from_pretrained(\"my_tokenizer\")\n",
        "\n",
        "# New text to tokenize\n",
        "text = \"Hello! This is a different sentence to tokenize.\"\n",
        "\n",
        "# Tokenize and get token IDs\n",
        "tokens = fast_tokenizer.tokenize(text)\n",
        "# print([t.replace(\"ƒ†\", \"\") for t in tokens])\n",
        "token_ids = fast_tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "print(\"Tokens:\", tokens)\n",
        "print(\"Token IDs:\", token_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `ƒ†` character you're seeing in the token strings (e.g., `'ƒ†This'`, `'ƒ†different'`, `'ƒ†sentence'`) is **not an actual character in the final tokenized text**‚Äîit's just a **display artifact** from how some tokenizers (especially Byte-Level BPE tokenizers and GPT-style tokenizers) represent **word boundaries**.\n",
        "\n",
        "### üîç What `ƒ†` Means\n",
        "\n",
        "* `ƒ†` (Unicode U+0120) is used by some tokenizers to indicate that the token starts with a **space**.\n",
        "* It helps distinguish between:\n",
        "\n",
        "  * `\"ƒ†token\"` ‚Üí the word \"token\" preceded by a space.\n",
        "  * `\"token\"` ‚Üí the word \"token\" without a preceding space (e.g., part of a longer word like `\"notoken\"`).\n",
        "\n",
        "### ‚úÖ Why It's Useful\n",
        "\n",
        "This distinction allows the tokenizer to:\n",
        "\n",
        "* **Model spaces explicitly**, which is important for text generation and reconstruction.\n",
        "* **Preserve the structure of the input text** more accurately during training.\n",
        "\n",
        "### üßº If You Want to Remove `ƒ†` from Display\n",
        "\n",
        "Just strip it during display if it's confusing:\n",
        "\n",
        "```python\n",
        "print([t.replace(\"ƒ†\", \"\") for t in tokens])\n",
        "```\n",
        "\n",
        "### ‚ö†Ô∏è But: Don't Remove It Before Encoding\n",
        "\n",
        "The tokenizer uses `ƒ†` internally to tokenize and reconstruct text correctly. You should **not** remove or alter it in any preprocessing unless you're just cleaning up **display/output**.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# creating a tokenizer on a HF dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Creating CSV from Arrow format: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:00<00:00, 66.45ba/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset saved to: data/questions_and_answers.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "import os\n",
        "\n",
        "dataset = load_dataset(\"SoorajK1/questions_and_answers\")\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "dataset[\"train\"].to_csv(\"data/questions_and_answers.csv\", index=False)\n",
        "print(\"Dataset saved to: data/questions_and_answers.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import unicodedata\n",
        "import pandas as pd\n",
        "\n",
        "def normalize_text(text: str) -> str:\n",
        "    text = unicodedata.normalize(\"NFKC\", text)\n",
        "    text = text.replace(\"‚Äô\", \"'\").replace(\"‚Äú\", '\"').replace(\"‚Äù\", '\"')\n",
        "    text = text.replace(\"‚Äì\", \"-\").replace(\"‚Äî\", \"-\")\n",
        "    text = re.sub(r\"[^\\x00-\\x7F]+\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['Unnamed: 0', 'question', 'answer', 'content_row'], dtype='object')"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pd.read_csv(\"data/questions_and_answers.csv\").columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 29438 samples from Q&A dataset\n",
            "Example: What is the purpose of the Android App mentioned? The Android App mentioned in the text is designed to collect data using smartphones. It serves as a tool that allows farmers to input and store information related to their farming activities.\n"
          ]
        }
      ],
      "source": [
        "def load_corpus_from_csv(path: str) -> list:\n",
        "    df = pd.read_csv(path)\n",
        "    # Combine questions and answers\n",
        "    combined_text = (df[\"question\"].astype(str) + \"\\n\" + df[\"answer\"].astype(str)).tolist()\n",
        "    combined_text = [normalize_text(t) for t in combined_text if t.strip()]\n",
        "    return combined_text\n",
        "\n",
        "corpus = load_corpus_from_csv(\"data/questions_and_answers.csv\")\n",
        "print(f\"Loaded {len(corpus)} samples from Q&A dataset\")\n",
        "print(f\"Example: {corpus[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "‚úÖ Tokenizer trained and saved to 'my_qa_tokenizer'\n"
          ]
        }
      ],
      "source": [
        "from tokenizers import ByteLevelBPETokenizer\n",
        "from tokenizers.pre_tokenizers import ByteLevel\n",
        "\n",
        "def train_tokenizer(corpus: list, output_dir: str, vocab_size: int = 20000) -> ByteLevelBPETokenizer:\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    tokenizer = ByteLevelBPETokenizer()\n",
        "    tokenizer._tokenizer.pre_tokenizer = ByteLevel()\n",
        "\n",
        "    tokenizer.train_from_iterator(\n",
        "        corpus,\n",
        "        vocab_size=vocab_size,\n",
        "        min_frequency=2,\n",
        "        show_progress=True,\n",
        "        special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"]\n",
        "    )\n",
        "    tokenizer.save_model(output_dir)\n",
        "    tokenizer.save(f\"{output_dir}/tokenizer.json\")\n",
        "    return tokenizer\n",
        "\n",
        "tokenizer = train_tokenizer(corpus, \"my_qa_tokenizer\")\n",
        "print(\"‚úÖ Tokenizer trained and saved to 'my_qa_tokenizer'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample text: Yes, the Android App can be used offline, without requiring an internet connection for data collection.     Farmers can input data into the app even in remote areas where internet access may be limited.     The data collected offline can be synced and uploaded to the app's server once an internet connection becomes available.\n",
            "Tokens: ['Y', 'es', ',', 'the', 'Android', 'App', 'can', 'be', 'used', 'off', 'line', ',', 'without', 'requiring', 'an', 'internet', 'connection', 'for', 'data', 'collection', '.', '', '', '', '', 'Farmers', 'can', 'input', 'data', 'into', 'the', 'app', 'even', 'in', 'remote', 'areas', 'where', 'internet', 'access', 'may', 'be', 'limited', '.', '', '', '', '', 'The', 'data', 'collected', 'off', 'line', 'can', 'be', 'syn', 'ced', 'and', 'uploaded', 'to', 'the', 'app', \"'s\", 'ser', 'ver', 'once', 'an', 'internet', 'connection', 'becomes', 'available', '.']\n",
            "Token IDs: [61, 275, 16, 266, 8822, 2619, 342, 312, 524, 1078, 3138, 16, 1365, 8573, 286, 13133, 9414, 309, 1537, 4598, 18, 225, 225, 225, 225, 1376, 342, 1492, 1537, 823, 266, 434, 1768, 287, 6746, 1086, 1271, 13133, 1472, 698, 312, 2717, 18, 225, 225, 225, 225, 320, 1537, 3201, 1078, 3138, 342, 312, 10010, 1900, 292, 8484, 295, 266, 434, 821, 3352, 403, 4494, 286, 13133, 9414, 2751, 1316, 18]\n"
          ]
        }
      ],
      "source": [
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "fast_tokenizer = PreTrainedTokenizerFast(\n",
        "    tokenizer_file=\"my_qa_tokenizer/tokenizer.json\",\n",
        "    unk_token=\"<unk>\",\n",
        "    pad_token=\"<pad>\",\n",
        "    bos_token=\"<s>\",\n",
        "    eos_token=\"</s>\",\n",
        "    mask_token=\"<mask>\"\n",
        ")\n",
        "\n",
        "sample_text = \"Yes, the Android App can be used offline, without requiring an internet connection for data collection. \\\n",
        "    Farmers can input data into the app even in remote areas where internet access may be limited. \\\n",
        "    The data collected offline can be synced and uploaded to the app's server once an internet connection becomes available.\"\n",
        "tokens = fast_tokenizer.tokenize(sample_text)\n",
        "ids = fast_tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "print(\"Sample text:\", sample_text)\n",
        "print(\"Tokens:\", [t.replace(\"ƒ†\", \"\") for t in tokens])\n",
        "print(\"Token IDs:\", ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No files have been modified since last commit. Skipping to prevent empty commit.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Tokenizer pushed: https://huggingface.co/mushfiqurrobin/qa-tokenizer-bpe\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import HfApi\n",
        "\n",
        "HF_USERNAME = HfApi().whoami()[\"name\"]\n",
        "REPO_NAME = \"qa-tokenizer-bpe\"\n",
        "TOKENIZER_REPO = f\"{HF_USERNAME}/{REPO_NAME}\"\n",
        "\n",
        "fast_tokenizer.save_pretrained(\"my_qa_tokenizer\")  # includes config files\n",
        "fast_tokenizer.push_to_hub(REPO_NAME)\n",
        "\n",
        "print(f\"‚úÖ Tokenizer pushed: https://huggingface.co/{TOKENIZER_REPO}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokens: ['B', 'uilding', 'a', 'to', 'ken', 'izer', 'from', 'scrat', 'ch', 'might', 'seem', 'd', 'a', 'unting', 'at', 'first', ',', 'but', 'with', 'the', 'right', 'tools', 'and', 'a', 'step', '-', 'by', '-', 'step', 'approach', ',', 'it', \"'s\", 'quite', 'manageable', '.']\n",
            "Token IDs: [38, 10743, 264, 295, 3743, 7545, 453, 14266, 359, 5625, 15894, 294, 69, 12043, 477, 1273, 16, 1418, 417, 266, 2379, 5852, 292, 264, 2011, 17, 3723, 17, 13507, 1926, 16, 376, 821, 12520, 15040, 18]\n"
          ]
        }
      ],
      "source": [
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "# Load the tokenizer from the directory\n",
        "fast_tokenizer = PreTrainedTokenizerFast.from_pretrained(\"my_qa_tokenizer\")\n",
        "\n",
        "text = \"Building a tokenizer from scratch might seem daunting at first, but with the right tools and a step-by-step approach, it's quite manageable.\"\n",
        "\n",
        "tokens = fast_tokenizer.tokenize(text)\n",
        "token_ids = fast_tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "print(\"Tokens:\", [t.replace(\"ƒ†\", \"\") for t in tokens])\n",
        "print(\"Token IDs:\", token_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# fine tuning a model with the tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "tokenizer = PreTrainedTokenizerFast(\n",
        "    tokenizer_file=\"my_qa_tokenizer/tokenizer.json\",\n",
        "    unk_token=\"<unk>\",\n",
        "    pad_token=\"<pad>\",\n",
        "    bos_token=\"<s>\",\n",
        "    eos_token=\"</s>\",\n",
        "    mask_token=\"<mask>\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 29438 samples from Q&A dataset\n",
            "Example: What is the purpose of the Android App mentioned? The Android App mentioned in the text is designed to collect data using smartphones. It serves as a tool that allows farmers to input and store information related to their farming activities.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import unicodedata\n",
        "import pandas as pd\n",
        "\n",
        "def normalize_text(text: str) -> str:\n",
        "    text = unicodedata.normalize(\"NFKC\", text)\n",
        "    text = text.replace(\"‚Äô\", \"'\").replace(\"‚Äú\", '\"').replace(\"‚Äù\", '\"')\n",
        "    text = text.replace(\"‚Äì\", \"-\").replace(\"‚Äî\", \"-\")\n",
        "    text = re.sub(r\"[^\\x00-\\x7F]+\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "def load_corpus_from_csv(path: str) -> list:\n",
        "    df = pd.read_csv(path)\n",
        "    # Combine questions and answers\n",
        "    combined_text = (df[\"question\"].astype(str) + \"\\n\" + df[\"answer\"].astype(str)).tolist()\n",
        "    combined_text = [normalize_text(t) for t in combined_text if t.strip()]\n",
        "    return combined_text\n",
        "\n",
        "corpus = load_corpus_from_csv(\"data/questions_and_answers.csv\")\n",
        "print(f\"Loaded {len(corpus)} samples from Q&A dataset\")\n",
        "print(f\"Example: {corpus[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': [59, 302, 298, 266, 655, 282, 266, 8822, 2619, 1396, 35, 320, 8822, 2619, 1396, 287, 266, 1017, 298, 3429, 295, 1761, 1537, 770, 10322, 18356, 18, 485, 3102, 366, 264, 8696, 397, 1186, 537, 295, 1492, 292, 4553, 1267, 1892, 295, 497, 551, 1191, 18, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
          ]
        }
      ],
      "source": [
        "# Tokenize the dataset\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples,\n",
        "        truncation=True,\n",
        "        max_length=128,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "\n",
        "tokenized_dataset = list(map(tokenize_function, corpus))\n",
        "print(tokenized_dataset[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert to HuggingFace Dataset\n",
        "from datasets import Dataset\n",
        "\n",
        "dataset_dict = {\n",
        "    \"input_ids\": [x[\"input_ids\"] for x in tokenized_dataset],\n",
        "    \"attention_mask\": [x[\"attention_mask\"] for x in tokenized_dataset],\n",
        "}\n",
        "\n",
        "train_dataset = Dataset.from_dict(dataset_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üß† Context:\n",
        "\n",
        "You're preparing a dataset for training a model using Hugging Face‚Äôs `transformers` library ‚Äî and more specifically, using their `Trainer` API. For that, your data must be in a `datasets.Dataset` format (from the `datasets` library, also known as ü§ó Datasets).\n",
        "\n",
        "---\n",
        "\n",
        "### üîç Line-by-line explanation:\n",
        "\n",
        "```python\n",
        "from datasets import Dataset\n",
        "```\n",
        "\n",
        "* This imports the `Dataset` class from the ü§ó `datasets` library.\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "dataset_dict = {\n",
        "    \"input_ids\": [x[\"input_ids\"] for x in tokenized_dataset],\n",
        "    \"attention_mask\": [x[\"attention_mask\"] for x in tokenized_dataset],\n",
        "}\n",
        "```\n",
        "\n",
        "* `tokenized_dataset` is likely a list of tokenized examples, where each example is a dictionary with keys like `\"input_ids\"` and `\"attention_mask\"` ‚Äî output from a tokenizer.\n",
        "* This line is **constructing a dictionary** where:\n",
        "\n",
        "  * Each key (e.g. `\"input_ids\"`) maps to a list of values ‚Äî one per example.\n",
        "  * You're restructuring the data into **columnar format**, i.e., `{\"input_ids\": [...], \"attention_mask\": [...]}` instead of a list of dicts.\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "train_dataset = Dataset.from_dict(dataset_dict)\n",
        "```\n",
        "\n",
        "* This converts the `dataset_dict` into a Hugging Face `Dataset` object.\n",
        "* Now `train_dataset` is a proper `datasets.Dataset`, which you can use with `Trainer`, apply `.map()` for transformations, shuffle it, etc.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Why do this?\n",
        "\n",
        "Hugging Face‚Äôs `Trainer` expects a `datasets.Dataset` object for training ‚Äî not just a list of Python dicts. This transformation allows seamless integration with Hugging Face‚Äôs training utilities.\n",
        "\n",
        "---\n",
        "\n",
        "### üß™ Example Input/Output\n",
        "\n",
        "If `tokenized_dataset` looked like:\n",
        "\n",
        "```python\n",
        "[\n",
        "  {'input_ids': [101, 102], 'attention_mask': [1, 1]},\n",
        "  {'input_ids': [101, 103], 'attention_mask': [1, 1]}\n",
        "]\n",
        "```\n",
        "\n",
        "Then `dataset_dict` becomes:\n",
        "\n",
        "```python\n",
        "{\n",
        "  'input_ids': [[101, 102], [101, 103]],\n",
        "  'attention_mask': [[1, 1], [1, 1]]\n",
        "}\n",
        "```\n",
        "\n",
        "Which is then converted to a `Dataset` object you can train on.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define GPT-2 config and model\n",
        "from transformers import GPT2Config, GPT2LMHeadModel\n",
        "\n",
        "config = GPT2Config(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    bos_token_id=tokenizer.bos_token_id,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    n_positions=128,\n",
        "    n_ctx=128,\n",
        "    n_embd=256,\n",
        "    n_layer=4,\n",
        "    n_head=4,\n",
        ")\n",
        "\n",
        "model = GPT2LMHeadModel(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This cell is **defining and initializing a custom GPT-2 model configuration and model**, tailored for a smaller, resource-friendly version of GPT-2. Here's a breakdown:\n",
        "\n",
        "---\n",
        "\n",
        "### üì¶ Imports\n",
        "\n",
        "```python\n",
        "from transformers import GPT2Config, GPT2LMHeadModel\n",
        "```\n",
        "\n",
        "* `GPT2Config`: lets you **customize model hyperparameters** (like number of layers, embedding size, etc).\n",
        "* `GPT2LMHeadModel`: the GPT-2 model class **with a language modeling (LM) head**, used for **causal language modeling** tasks (predicting next token given previous tokens).\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öôÔ∏è Define Custom GPT-2 Configuration\n",
        "\n",
        "```python\n",
        "config = GPT2Config(\n",
        "    vocab_size=tokenizer.vocab_size,        # Size of the vocabulary (must match tokenizer)\n",
        "    bos_token_id=tokenizer.bos_token_id,    # Beginning of sequence token ID\n",
        "    eos_token_id=tokenizer.eos_token_id,    # End of sequence token ID\n",
        "    n_positions=128,                        # Max sequence length\n",
        "    n_ctx=128,                              # Max context window (same as above)\n",
        "    n_embd=256,                             # Embedding size (how big each token vector is)\n",
        "    n_layer=4,                              # Number of transformer layers (depth)\n",
        "    n_head=4                                # Number of attention heads\n",
        ")\n",
        "```\n",
        "\n",
        "‚úÖ You're **defining a \"mini GPT-2\"** ‚Äî small enough to train quickly on limited hardware (ideal for fine-tuning on custom or educational datasets).\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Instantiate the Model\n",
        "\n",
        "```python\n",
        "model = GPT2LMHeadModel(config)\n",
        "```\n",
        "\n",
        "* This creates a **GPT-2 model** initialized **from scratch** (i.e., not loaded from pre-trained weights) using your custom config.\n",
        "* It's a **decoder-only transformer** for **causal language modeling**, suitable for tasks like text generation, next-token prediction, etc.\n",
        "\n",
        "---\n",
        "\n",
        "### Summary\n",
        "\n",
        "| Component     | Value Set Here | What It Means                        |\n",
        "| ------------- | -------------- | ------------------------------------ |\n",
        "| `n_positions` | 128            | Max sequence length (tokens)         |\n",
        "| `n_embd`      | 256            | Dimensionality of token embeddings   |\n",
        "| `n_layer`     | 4              | Number of transformer blocks         |\n",
        "| `n_head`      | 4              | Multi-head attention heads           |\n",
        "| `vocab_size`  | From tokenizer | Vocabulary size the model can handle |\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ This is ideal when:\n",
        "\n",
        "* You're training GPT-2 **from scratch** on a small dataset.\n",
        "* You want a **smaller, faster, cheaper** model for prototyping or testing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Probable parameter counts:\n",
        "\n",
        "---\n",
        "\n",
        "### üîß Model Configuration Recap\n",
        "\n",
        "* `n_layer = 4`\n",
        "* `n_head = 4`\n",
        "* `n_embd = 256`\n",
        "* `n_ctx = 128`\n",
        "* `vocab_size = 20,000`\n",
        "\n",
        "---\n",
        "\n",
        "### üì¶ Parameter Count by Component\n",
        "\n",
        "#### 1. **Embeddings**\n",
        "\n",
        "* **Token Embeddings**: `vocab_size √ó n_embd = 20,000 √ó 256 = 5,120,000`\n",
        "* **Positional Embeddings**: `n_ctx √ó n_embd = 128 √ó 256 = 32,768`\n",
        "\n",
        "‚úÖ **Embeddings Total = \\~5.15M**\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. **Transformer Blocks (x4 layers)**\n",
        "\n",
        "Each layer has:\n",
        "\n",
        "* **Self-Attention**: 4 weight matrices (Q, K, V, Out), each `n_embd √ó n_embd = 256 √ó 256 = 65,536`\n",
        "\n",
        "  * Total = 4 √ó 65,536 = 262,144\n",
        "* **Feedforward MLP**:\n",
        "\n",
        "  * First layer: `256 √ó 1024 = 262,144`\n",
        "  * Second layer: `1024 √ó 256 = 262,144`\n",
        "  * Total = 524,288\n",
        "\n",
        "‚û°Ô∏è **Per Layer Total** ‚âà 262K + 524K = **786,432**\n",
        "‚û°Ô∏è **4 Layers Total** = 4 √ó 786,432 = **3,145,728**\n",
        "\n",
        "---\n",
        "\n",
        "#### 3. **Final LayerNorm**: \\~256 parameters (very small)\n",
        "\n",
        "---\n",
        "\n",
        "#### 4. **LM Head**: Tied with token embeddings, no extra params.\n",
        "\n",
        "---\n",
        "\n",
        "### üìä Final Total Parameter Count\n",
        "\n",
        "| Component   | Parameters        |\n",
        "| ----------- | ----------------- |\n",
        "| Embeddings  | 5,152,768         |\n",
        "| Transformer | 3,145,728         |\n",
        "| LayerNorm   | \\~256             |\n",
        "| **Total**   | **\\~8.3 million** |\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Summary\n",
        "\n",
        "> ‚úÖ **Your GPT-2 model has \\~8.3 million parameters** with a vocab size of 20,000 and 4 transformer layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch: 2.6.0+cu124\n",
            "transformers: 4.52.4\n",
            "accelerate: 1.7.0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import transformers\n",
        "import accelerate\n",
        "\n",
        "print(\"torch:\", torch.__version__)\n",
        "print(\"transformers:\", transformers.__version__)\n",
        "print(\"accelerate:\", accelerate.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define training arguments\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2-custom\",\n",
        "    overwrite_output_dir=True,\n",
        "    per_device_train_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=20,\n",
        "    save_steps=50,\n",
        "    save_total_limit=2,\n",
        "    fp16=True,\n",
        "    report_to=\"none\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This cell defines a set of **training configurations** for fine-tuning your custom GPT-2 model using Hugging Face's `Trainer` API. It uses the `TrainingArguments` class from the `transformers` library.\n",
        "\n",
        "---\n",
        "\n",
        "## üîß Breakdown of Each Argument\n",
        "\n",
        "```python\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2-custom\",\n",
        "```\n",
        "\n",
        "* üìÅ Where to save the model checkpoints, logs, etc.\n",
        "\n",
        "```python\n",
        "    overwrite_output_dir=True,\n",
        "```\n",
        "\n",
        "* ‚úÖ If this directory already exists, it will be **overwritten**. Useful when you're re-training.\n",
        "\n",
        "```python\n",
        "    per_device_train_batch_size=8,\n",
        "```\n",
        "\n",
        "* üß† Number of examples processed **per GPU/CPU** in a single forward/backward pass.\n",
        "* If you're using 1 GPU: batch size = 8\n",
        "* If using 2 GPUs: effective batch size = 8 √ó 2 = 16\n",
        "\n",
        "```python\n",
        "    num_train_epochs=3,\n",
        "```\n",
        "\n",
        "* üîÅ Train the entire dataset **3 times** (epochs)\n",
        "\n",
        "```python\n",
        "    logging_dir=\"./logs\",\n",
        "```\n",
        "\n",
        "* üìì Directory to save logs for TensorBoard or manual inspection\n",
        "\n",
        "```python\n",
        "    logging_steps=20,\n",
        "```\n",
        "\n",
        "* ü™µ Log training metrics (loss, learning rate, etc.) every 20 steps\n",
        "\n",
        "```python\n",
        "    save_steps=50,\n",
        "```\n",
        "\n",
        "* üíæ Save a model checkpoint every 50 steps\n",
        "\n",
        "```python\n",
        "    save_total_limit=2,\n",
        "```\n",
        "\n",
        "* üíº Keep only the **2 most recent checkpoints** to save disk space\n",
        "* Older checkpoints are automatically deleted\n",
        "\n",
        "```python\n",
        "    fp16=True,\n",
        "```\n",
        "\n",
        "* üßÆ Enables **mixed-precision (FP16)** training, reducing memory usage and speeding up training on supported GPUs\n",
        "\n",
        "```python\n",
        "    report_to=\"none\"\n",
        ")\n",
        "```\n",
        "\n",
        "* üõë Disables reporting to external logging systems (like WandB, Comet, etc.)\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Why This Is Useful\n",
        "\n",
        "This config:\n",
        "\n",
        "* Is compact and memory-efficient (good for small models or laptops)\n",
        "* Saves space by limiting checkpoint storage\n",
        "* Enables fast training with FP16\n",
        "* Prepares you to use `Trainer(...)` without manually handling loops, device setup, or logging\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_494979/543745566.py:8: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='11040' max='11040' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [11040/11040 1:21:04, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>9.342100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>8.920000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>8.656300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>8.329100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>8.047700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>7.831200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>7.671200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>7.387600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>7.248400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>7.106700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>7.009900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>6.852600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>6.781700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>6.644600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>6.522400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>6.510700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>6.476600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>6.442000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>6.368400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>6.284100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>6.306400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>6.335700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>6.277400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>6.279800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>6.315400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>520</td>\n",
              "      <td>6.262800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>540</td>\n",
              "      <td>6.085100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>560</td>\n",
              "      <td>6.191500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>580</td>\n",
              "      <td>6.129200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>6.064300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>620</td>\n",
              "      <td>6.131900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>640</td>\n",
              "      <td>6.031400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>660</td>\n",
              "      <td>6.076200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>680</td>\n",
              "      <td>6.000900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>6.032900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>720</td>\n",
              "      <td>6.025500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>740</td>\n",
              "      <td>6.032100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>760</td>\n",
              "      <td>6.014400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>780</td>\n",
              "      <td>6.016100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>5.953000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>820</td>\n",
              "      <td>5.967200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>840</td>\n",
              "      <td>5.924600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>860</td>\n",
              "      <td>5.912400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>880</td>\n",
              "      <td>5.905300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>6.004900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>920</td>\n",
              "      <td>5.945800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>940</td>\n",
              "      <td>5.909700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>960</td>\n",
              "      <td>5.855200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>980</td>\n",
              "      <td>5.865000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>5.812500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1020</td>\n",
              "      <td>5.938800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1040</td>\n",
              "      <td>5.854200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1060</td>\n",
              "      <td>5.872200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1080</td>\n",
              "      <td>5.788700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>5.778900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1120</td>\n",
              "      <td>5.779800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1140</td>\n",
              "      <td>5.757800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1160</td>\n",
              "      <td>5.842400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1180</td>\n",
              "      <td>5.840300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>5.809300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1220</td>\n",
              "      <td>5.670900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1240</td>\n",
              "      <td>5.748500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1260</td>\n",
              "      <td>5.719700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1280</td>\n",
              "      <td>5.626500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>5.703700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1320</td>\n",
              "      <td>5.694900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1340</td>\n",
              "      <td>5.710100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1360</td>\n",
              "      <td>5.713300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1380</td>\n",
              "      <td>5.637800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>5.712900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1420</td>\n",
              "      <td>5.763400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1440</td>\n",
              "      <td>5.577300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1460</td>\n",
              "      <td>5.711200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1480</td>\n",
              "      <td>5.681600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>5.605200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1520</td>\n",
              "      <td>5.678100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1540</td>\n",
              "      <td>5.643800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1560</td>\n",
              "      <td>5.638600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1580</td>\n",
              "      <td>5.592200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>5.520700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1620</td>\n",
              "      <td>5.595100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1640</td>\n",
              "      <td>5.589800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1660</td>\n",
              "      <td>5.538100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1680</td>\n",
              "      <td>5.657300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>5.402300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1720</td>\n",
              "      <td>5.591500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1740</td>\n",
              "      <td>5.504700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1760</td>\n",
              "      <td>5.611300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1780</td>\n",
              "      <td>5.528900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>5.516900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1820</td>\n",
              "      <td>5.575700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1840</td>\n",
              "      <td>5.535900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1860</td>\n",
              "      <td>5.614200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1880</td>\n",
              "      <td>5.546200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>5.513200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1920</td>\n",
              "      <td>5.425200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1940</td>\n",
              "      <td>5.511400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1960</td>\n",
              "      <td>5.414200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1980</td>\n",
              "      <td>5.434300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>5.424200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2020</td>\n",
              "      <td>5.500500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2040</td>\n",
              "      <td>5.475900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2060</td>\n",
              "      <td>5.439000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2080</td>\n",
              "      <td>5.456400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>5.390800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2120</td>\n",
              "      <td>5.421000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2140</td>\n",
              "      <td>5.388700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2160</td>\n",
              "      <td>5.375600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2180</td>\n",
              "      <td>5.361800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>5.412500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2220</td>\n",
              "      <td>5.497100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2240</td>\n",
              "      <td>5.377300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2260</td>\n",
              "      <td>5.305000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2280</td>\n",
              "      <td>5.449900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>5.405000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2320</td>\n",
              "      <td>5.410800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2340</td>\n",
              "      <td>5.504300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2360</td>\n",
              "      <td>5.387800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2380</td>\n",
              "      <td>5.344900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>5.452700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2420</td>\n",
              "      <td>5.398100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2440</td>\n",
              "      <td>5.315200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2460</td>\n",
              "      <td>5.440700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2480</td>\n",
              "      <td>5.296000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>5.395400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2520</td>\n",
              "      <td>5.316000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2540</td>\n",
              "      <td>5.376200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2560</td>\n",
              "      <td>5.399700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2580</td>\n",
              "      <td>5.326300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>5.293400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2620</td>\n",
              "      <td>5.354500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2640</td>\n",
              "      <td>5.365000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2660</td>\n",
              "      <td>5.349300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2680</td>\n",
              "      <td>5.343600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2700</td>\n",
              "      <td>5.281600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2720</td>\n",
              "      <td>5.287600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2740</td>\n",
              "      <td>5.289800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2760</td>\n",
              "      <td>5.336000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2780</td>\n",
              "      <td>5.357300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2800</td>\n",
              "      <td>5.306700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2820</td>\n",
              "      <td>5.275800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2840</td>\n",
              "      <td>5.289100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2860</td>\n",
              "      <td>5.238500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2880</td>\n",
              "      <td>5.441000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2900</td>\n",
              "      <td>5.199400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2920</td>\n",
              "      <td>5.211600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2940</td>\n",
              "      <td>5.318000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2960</td>\n",
              "      <td>5.284600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2980</td>\n",
              "      <td>5.241800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>5.247700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3020</td>\n",
              "      <td>5.253300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3040</td>\n",
              "      <td>5.238000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3060</td>\n",
              "      <td>5.336200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3080</td>\n",
              "      <td>5.115400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3100</td>\n",
              "      <td>5.218400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3120</td>\n",
              "      <td>5.257500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3140</td>\n",
              "      <td>5.264300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3160</td>\n",
              "      <td>5.145000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3180</td>\n",
              "      <td>5.228200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3200</td>\n",
              "      <td>5.216200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3220</td>\n",
              "      <td>5.120500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3240</td>\n",
              "      <td>5.300100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3260</td>\n",
              "      <td>5.189600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3280</td>\n",
              "      <td>5.184700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3300</td>\n",
              "      <td>5.256700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3320</td>\n",
              "      <td>5.204500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3340</td>\n",
              "      <td>5.158900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3360</td>\n",
              "      <td>5.228500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3380</td>\n",
              "      <td>5.215100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3400</td>\n",
              "      <td>5.144600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3420</td>\n",
              "      <td>5.111000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3440</td>\n",
              "      <td>5.094700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3460</td>\n",
              "      <td>5.268800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3480</td>\n",
              "      <td>5.141900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>5.182100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3520</td>\n",
              "      <td>5.106100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3540</td>\n",
              "      <td>5.155800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3560</td>\n",
              "      <td>5.142900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3580</td>\n",
              "      <td>5.083100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3600</td>\n",
              "      <td>5.237100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3620</td>\n",
              "      <td>5.169500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3640</td>\n",
              "      <td>5.169500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3660</td>\n",
              "      <td>5.056300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3680</td>\n",
              "      <td>5.071200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3700</td>\n",
              "      <td>5.118900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3720</td>\n",
              "      <td>5.093700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3740</td>\n",
              "      <td>5.006800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3760</td>\n",
              "      <td>5.042000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3780</td>\n",
              "      <td>5.122700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3800</td>\n",
              "      <td>5.014200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3820</td>\n",
              "      <td>5.071000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3840</td>\n",
              "      <td>4.957900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3860</td>\n",
              "      <td>5.037000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3880</td>\n",
              "      <td>5.069500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3900</td>\n",
              "      <td>5.124500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3920</td>\n",
              "      <td>5.145900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3940</td>\n",
              "      <td>5.003000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3960</td>\n",
              "      <td>4.956300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3980</td>\n",
              "      <td>5.071300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>4.999600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4020</td>\n",
              "      <td>5.048900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4040</td>\n",
              "      <td>5.056700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4060</td>\n",
              "      <td>5.089800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4080</td>\n",
              "      <td>5.044100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4100</td>\n",
              "      <td>5.120500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4120</td>\n",
              "      <td>5.022400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4140</td>\n",
              "      <td>4.986200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4160</td>\n",
              "      <td>5.010800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4180</td>\n",
              "      <td>5.046000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4200</td>\n",
              "      <td>5.074500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4220</td>\n",
              "      <td>4.985300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4240</td>\n",
              "      <td>4.968200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4260</td>\n",
              "      <td>5.061900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4280</td>\n",
              "      <td>5.016700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4300</td>\n",
              "      <td>4.981600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4320</td>\n",
              "      <td>4.966700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4340</td>\n",
              "      <td>4.976300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4360</td>\n",
              "      <td>4.978600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4380</td>\n",
              "      <td>5.022300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4400</td>\n",
              "      <td>4.948200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4420</td>\n",
              "      <td>5.011200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4440</td>\n",
              "      <td>4.981200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4460</td>\n",
              "      <td>5.038200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4480</td>\n",
              "      <td>4.958600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>4.973100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4520</td>\n",
              "      <td>5.051400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4540</td>\n",
              "      <td>5.038200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4560</td>\n",
              "      <td>5.006500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4580</td>\n",
              "      <td>5.008400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4600</td>\n",
              "      <td>4.999700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4620</td>\n",
              "      <td>5.025300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4640</td>\n",
              "      <td>4.998700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4660</td>\n",
              "      <td>4.941900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4680</td>\n",
              "      <td>4.975600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4700</td>\n",
              "      <td>4.863300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4720</td>\n",
              "      <td>4.942500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4740</td>\n",
              "      <td>4.959300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4760</td>\n",
              "      <td>4.999300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4780</td>\n",
              "      <td>4.970800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4800</td>\n",
              "      <td>4.965700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4820</td>\n",
              "      <td>4.935600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4840</td>\n",
              "      <td>4.957000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4860</td>\n",
              "      <td>4.968600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4880</td>\n",
              "      <td>4.918300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4900</td>\n",
              "      <td>4.945500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4920</td>\n",
              "      <td>4.920800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4940</td>\n",
              "      <td>4.903800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4960</td>\n",
              "      <td>5.018600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4980</td>\n",
              "      <td>4.870900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>4.910900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5020</td>\n",
              "      <td>4.937100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5040</td>\n",
              "      <td>4.976100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5060</td>\n",
              "      <td>4.854300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5080</td>\n",
              "      <td>4.849500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5100</td>\n",
              "      <td>5.008600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5120</td>\n",
              "      <td>4.864900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5140</td>\n",
              "      <td>4.866500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5160</td>\n",
              "      <td>5.047600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5180</td>\n",
              "      <td>4.895500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5200</td>\n",
              "      <td>4.937700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5220</td>\n",
              "      <td>4.917600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5240</td>\n",
              "      <td>4.857800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5260</td>\n",
              "      <td>4.941400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5280</td>\n",
              "      <td>4.846000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5300</td>\n",
              "      <td>4.820700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5320</td>\n",
              "      <td>4.790700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5340</td>\n",
              "      <td>4.952000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5360</td>\n",
              "      <td>4.858300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5380</td>\n",
              "      <td>4.788200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5400</td>\n",
              "      <td>4.828200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5420</td>\n",
              "      <td>5.003800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5440</td>\n",
              "      <td>4.860400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5460</td>\n",
              "      <td>4.830100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5480</td>\n",
              "      <td>4.952300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>4.845200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5520</td>\n",
              "      <td>4.854700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5540</td>\n",
              "      <td>4.882700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5560</td>\n",
              "      <td>4.825000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5580</td>\n",
              "      <td>4.902600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5600</td>\n",
              "      <td>4.819000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5620</td>\n",
              "      <td>4.874900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5640</td>\n",
              "      <td>4.895900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5660</td>\n",
              "      <td>4.740000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5680</td>\n",
              "      <td>4.834800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5700</td>\n",
              "      <td>4.937500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5720</td>\n",
              "      <td>4.863500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5740</td>\n",
              "      <td>4.874000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5760</td>\n",
              "      <td>4.946900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5780</td>\n",
              "      <td>4.964300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5800</td>\n",
              "      <td>4.869000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5820</td>\n",
              "      <td>4.836500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5840</td>\n",
              "      <td>4.752000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5860</td>\n",
              "      <td>4.865900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5880</td>\n",
              "      <td>4.808300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5900</td>\n",
              "      <td>4.845600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5920</td>\n",
              "      <td>4.860700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5940</td>\n",
              "      <td>4.791700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5960</td>\n",
              "      <td>4.882200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5980</td>\n",
              "      <td>4.795600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>4.891600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6020</td>\n",
              "      <td>4.824200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6040</td>\n",
              "      <td>4.884600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6060</td>\n",
              "      <td>4.863100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6080</td>\n",
              "      <td>4.749800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6100</td>\n",
              "      <td>4.806600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6120</td>\n",
              "      <td>4.827300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6140</td>\n",
              "      <td>4.810100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6160</td>\n",
              "      <td>4.767800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6180</td>\n",
              "      <td>4.849600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6200</td>\n",
              "      <td>4.801700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6220</td>\n",
              "      <td>4.956100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6240</td>\n",
              "      <td>4.785200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6260</td>\n",
              "      <td>4.832600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6280</td>\n",
              "      <td>4.712300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6300</td>\n",
              "      <td>4.852700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6320</td>\n",
              "      <td>4.857500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6340</td>\n",
              "      <td>4.680500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6360</td>\n",
              "      <td>4.822100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6380</td>\n",
              "      <td>4.760200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6400</td>\n",
              "      <td>4.719100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6420</td>\n",
              "      <td>4.790000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6440</td>\n",
              "      <td>4.885800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6460</td>\n",
              "      <td>4.863600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6480</td>\n",
              "      <td>4.814300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>4.848800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6520</td>\n",
              "      <td>4.863700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6540</td>\n",
              "      <td>4.771200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6560</td>\n",
              "      <td>4.752500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6580</td>\n",
              "      <td>4.857500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6600</td>\n",
              "      <td>4.785100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6620</td>\n",
              "      <td>4.752900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6640</td>\n",
              "      <td>4.749900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6660</td>\n",
              "      <td>4.793600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6680</td>\n",
              "      <td>4.727700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6700</td>\n",
              "      <td>4.731400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6720</td>\n",
              "      <td>4.739500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6740</td>\n",
              "      <td>4.830800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6760</td>\n",
              "      <td>4.816100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6780</td>\n",
              "      <td>4.741200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6800</td>\n",
              "      <td>4.768100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6820</td>\n",
              "      <td>4.755400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6840</td>\n",
              "      <td>4.740600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6860</td>\n",
              "      <td>4.813100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6880</td>\n",
              "      <td>4.868300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6900</td>\n",
              "      <td>4.747900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6920</td>\n",
              "      <td>4.752600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6940</td>\n",
              "      <td>4.694600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6960</td>\n",
              "      <td>4.757200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6980</td>\n",
              "      <td>4.787000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>4.699400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7020</td>\n",
              "      <td>4.774300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7040</td>\n",
              "      <td>4.704400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7060</td>\n",
              "      <td>4.696500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7080</td>\n",
              "      <td>4.778100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7100</td>\n",
              "      <td>4.782700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7120</td>\n",
              "      <td>4.695500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7140</td>\n",
              "      <td>4.876000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7160</td>\n",
              "      <td>4.667400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7180</td>\n",
              "      <td>4.759400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7200</td>\n",
              "      <td>4.802400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7220</td>\n",
              "      <td>4.783800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7240</td>\n",
              "      <td>4.828300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7260</td>\n",
              "      <td>4.755400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7280</td>\n",
              "      <td>4.663200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7300</td>\n",
              "      <td>4.759400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7320</td>\n",
              "      <td>4.603900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7340</td>\n",
              "      <td>4.769900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7360</td>\n",
              "      <td>4.863000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7380</td>\n",
              "      <td>4.705600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7400</td>\n",
              "      <td>4.604600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7420</td>\n",
              "      <td>4.687300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7440</td>\n",
              "      <td>4.735900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7460</td>\n",
              "      <td>4.672100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7480</td>\n",
              "      <td>4.659200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>4.706200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7520</td>\n",
              "      <td>4.696500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7540</td>\n",
              "      <td>4.645900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7560</td>\n",
              "      <td>4.661700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7580</td>\n",
              "      <td>4.751100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7600</td>\n",
              "      <td>4.802100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7620</td>\n",
              "      <td>4.743300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7640</td>\n",
              "      <td>4.641300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7660</td>\n",
              "      <td>4.570800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7680</td>\n",
              "      <td>4.709200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7700</td>\n",
              "      <td>4.743800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7720</td>\n",
              "      <td>4.700700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7740</td>\n",
              "      <td>4.648300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7760</td>\n",
              "      <td>4.654200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7780</td>\n",
              "      <td>4.621000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7800</td>\n",
              "      <td>4.634800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7820</td>\n",
              "      <td>4.585100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7840</td>\n",
              "      <td>4.536000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7860</td>\n",
              "      <td>4.593100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7880</td>\n",
              "      <td>4.678400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7900</td>\n",
              "      <td>4.630200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7920</td>\n",
              "      <td>4.780900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7940</td>\n",
              "      <td>4.629900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7960</td>\n",
              "      <td>4.802300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7980</td>\n",
              "      <td>4.652800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>4.625000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8020</td>\n",
              "      <td>4.639900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8040</td>\n",
              "      <td>4.549300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8060</td>\n",
              "      <td>4.539200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8080</td>\n",
              "      <td>4.575100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8100</td>\n",
              "      <td>4.658600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8120</td>\n",
              "      <td>4.726700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8140</td>\n",
              "      <td>4.669500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8160</td>\n",
              "      <td>4.634200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8180</td>\n",
              "      <td>4.663700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8200</td>\n",
              "      <td>4.759400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8220</td>\n",
              "      <td>4.695800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8240</td>\n",
              "      <td>4.662200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8260</td>\n",
              "      <td>4.660800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8280</td>\n",
              "      <td>4.572800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8300</td>\n",
              "      <td>4.674000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8320</td>\n",
              "      <td>4.669300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8340</td>\n",
              "      <td>4.726900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8360</td>\n",
              "      <td>4.704000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8380</td>\n",
              "      <td>4.651800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8400</td>\n",
              "      <td>4.651800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8420</td>\n",
              "      <td>4.655300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8440</td>\n",
              "      <td>4.654500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8460</td>\n",
              "      <td>4.632300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8480</td>\n",
              "      <td>4.671400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8500</td>\n",
              "      <td>4.527600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8520</td>\n",
              "      <td>4.620200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8540</td>\n",
              "      <td>4.726700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8560</td>\n",
              "      <td>4.724700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8580</td>\n",
              "      <td>4.623600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8600</td>\n",
              "      <td>4.623500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8620</td>\n",
              "      <td>4.648700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8640</td>\n",
              "      <td>4.628700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8660</td>\n",
              "      <td>4.583200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8680</td>\n",
              "      <td>4.556100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8700</td>\n",
              "      <td>4.636000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8720</td>\n",
              "      <td>4.648700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8740</td>\n",
              "      <td>4.598300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8760</td>\n",
              "      <td>4.673800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8780</td>\n",
              "      <td>4.624500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8800</td>\n",
              "      <td>4.621000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8820</td>\n",
              "      <td>4.681500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8840</td>\n",
              "      <td>4.671500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8860</td>\n",
              "      <td>4.658600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8880</td>\n",
              "      <td>4.676100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8900</td>\n",
              "      <td>4.492500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8920</td>\n",
              "      <td>4.613400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8940</td>\n",
              "      <td>4.600900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8960</td>\n",
              "      <td>4.686100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8980</td>\n",
              "      <td>4.546400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>4.665200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9020</td>\n",
              "      <td>4.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9040</td>\n",
              "      <td>4.548200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9060</td>\n",
              "      <td>4.658700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9080</td>\n",
              "      <td>4.598300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9100</td>\n",
              "      <td>4.580300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9120</td>\n",
              "      <td>4.755200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9140</td>\n",
              "      <td>4.586200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9160</td>\n",
              "      <td>4.571300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9180</td>\n",
              "      <td>4.665900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9200</td>\n",
              "      <td>4.608100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9220</td>\n",
              "      <td>4.654900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9240</td>\n",
              "      <td>4.559900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9260</td>\n",
              "      <td>4.667600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9280</td>\n",
              "      <td>4.693300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9300</td>\n",
              "      <td>4.529300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9320</td>\n",
              "      <td>4.556800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9340</td>\n",
              "      <td>4.525400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9360</td>\n",
              "      <td>4.691400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9380</td>\n",
              "      <td>4.597500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9400</td>\n",
              "      <td>4.661900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9420</td>\n",
              "      <td>4.667800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9440</td>\n",
              "      <td>4.601800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9460</td>\n",
              "      <td>4.685000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9480</td>\n",
              "      <td>4.700400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9500</td>\n",
              "      <td>4.606700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9520</td>\n",
              "      <td>4.622900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9540</td>\n",
              "      <td>4.525200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9560</td>\n",
              "      <td>4.596300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9580</td>\n",
              "      <td>4.571700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9600</td>\n",
              "      <td>4.622900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9620</td>\n",
              "      <td>4.471900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9640</td>\n",
              "      <td>4.484900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9660</td>\n",
              "      <td>4.721000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9680</td>\n",
              "      <td>4.641800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9700</td>\n",
              "      <td>4.580000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9720</td>\n",
              "      <td>4.581900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9740</td>\n",
              "      <td>4.623900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9760</td>\n",
              "      <td>4.621900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9780</td>\n",
              "      <td>4.596300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9800</td>\n",
              "      <td>4.607100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9820</td>\n",
              "      <td>4.475900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9840</td>\n",
              "      <td>4.578100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9860</td>\n",
              "      <td>4.604500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9880</td>\n",
              "      <td>4.599100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9900</td>\n",
              "      <td>4.611700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9920</td>\n",
              "      <td>4.629600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9940</td>\n",
              "      <td>4.575700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9960</td>\n",
              "      <td>4.621100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9980</td>\n",
              "      <td>4.591300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>4.526800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10020</td>\n",
              "      <td>4.590800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10040</td>\n",
              "      <td>4.654900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10060</td>\n",
              "      <td>4.531900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10080</td>\n",
              "      <td>4.569600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10100</td>\n",
              "      <td>4.719200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10120</td>\n",
              "      <td>4.528600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10140</td>\n",
              "      <td>4.491800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10160</td>\n",
              "      <td>4.572600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10180</td>\n",
              "      <td>4.612600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10200</td>\n",
              "      <td>4.628800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10220</td>\n",
              "      <td>4.499100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10240</td>\n",
              "      <td>4.577900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10260</td>\n",
              "      <td>4.718500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10280</td>\n",
              "      <td>4.623800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10300</td>\n",
              "      <td>4.648600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10320</td>\n",
              "      <td>4.628100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10340</td>\n",
              "      <td>4.543100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10360</td>\n",
              "      <td>4.561200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10380</td>\n",
              "      <td>4.596100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10400</td>\n",
              "      <td>4.612300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10420</td>\n",
              "      <td>4.584100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10440</td>\n",
              "      <td>4.583400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10460</td>\n",
              "      <td>4.618200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10480</td>\n",
              "      <td>4.600900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10500</td>\n",
              "      <td>4.480600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10520</td>\n",
              "      <td>4.592400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10540</td>\n",
              "      <td>4.587600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10560</td>\n",
              "      <td>4.468900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10580</td>\n",
              "      <td>4.622800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10600</td>\n",
              "      <td>4.585300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10620</td>\n",
              "      <td>4.637800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10640</td>\n",
              "      <td>4.621400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10660</td>\n",
              "      <td>4.623500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10680</td>\n",
              "      <td>4.543000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10700</td>\n",
              "      <td>4.589800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10720</td>\n",
              "      <td>4.600600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10740</td>\n",
              "      <td>4.618500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10760</td>\n",
              "      <td>4.517100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10780</td>\n",
              "      <td>4.498700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10800</td>\n",
              "      <td>4.658600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10820</td>\n",
              "      <td>4.558300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10840</td>\n",
              "      <td>4.630700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10860</td>\n",
              "      <td>4.647400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10880</td>\n",
              "      <td>4.647100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10900</td>\n",
              "      <td>4.590700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10920</td>\n",
              "      <td>4.585400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10940</td>\n",
              "      <td>4.575300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10960</td>\n",
              "      <td>4.620900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10980</td>\n",
              "      <td>4.523700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11000</td>\n",
              "      <td>4.631200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11020</td>\n",
              "      <td>4.659500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11040</td>\n",
              "      <td>4.658000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=11040, training_loss=5.080801551929419, metrics={'train_runtime': 4867.237, 'train_samples_per_second': 18.145, 'train_steps_per_second': 2.268, 'total_flos': 214297094651904.0, 'train_loss': 5.080801551929419, 'epoch': 3.0})"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Trainer API to fine-tune\n",
        "from transformers import Trainer, DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=False  # CLM = not MLM\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This cell fine-tunes your **custom GPT-2 model** using the Hugging Face `Trainer` API ‚Äî a high-level training loop abstraction that takes care of all the boilerplate (optimizer, scheduling, GPU management, etc.).\n",
        "\n",
        "---\n",
        "\n",
        "## üîç Explanation Line-by-Line\n",
        "\n",
        "### 1. **Importing Necessary Tools**\n",
        "\n",
        "```python\n",
        "from transformers import Trainer, DataCollatorForLanguageModeling\n",
        "```\n",
        "\n",
        "* `Trainer`: Core API for training Hugging Face models.\n",
        "* `DataCollatorForLanguageModeling`: Prepares batches of tokenized inputs for language modeling tasks (like GPT-2).\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Creating a Data Collator**\n",
        "\n",
        "```python\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=False  # CLM = not MLM\n",
        ")\n",
        "```\n",
        "\n",
        "* Tells the trainer **how to batch** sequences.\n",
        "* `mlm=False` ‚Üí you're doing **causal language modeling (CLM)**, **not** masked language modeling (MLM).\n",
        "\n",
        "  * MLM is used for BERT.\n",
        "  * CLM is used for GPT-style models.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Defining the Trainer**\n",
        "\n",
        "```python\n",
        "trainer = Trainer(\n",
        "    model=model,  # Your GPT-2 variant\n",
        "    args=training_args,  # Defined earlier\n",
        "    train_dataset=train_dataset,  # Tokenized & formatted data\n",
        "    tokenizer=tokenizer,  # Needed for decoding/evaluation\n",
        "    data_collator=data_collator  # Prepares padded, shifted batches\n",
        ")\n",
        "```\n",
        "\n",
        "* Sets up the full training pipeline using:\n",
        "\n",
        "  * Your model\n",
        "  * Your training config (`TrainingArguments`)\n",
        "  * Your tokenized training data\n",
        "  * A collator to create batches on the fly\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Training the Model**\n",
        "\n",
        "```python\n",
        "trainer.train()\n",
        "```\n",
        "\n",
        "* Begins training:\n",
        "\n",
        "  * Loads data in batches\n",
        "  * Runs forward and backward passes\n",
        "  * Applies optimizer updates\n",
        "  * Logs metrics\n",
        "  * Saves checkpoints\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Summary\n",
        "\n",
        "This cell:\n",
        "\n",
        "* ‚úÖ Automatically manages your training loop\n",
        "* ‚úÖ Uses GPT-2 style language modeling (next token prediction)\n",
        "* ‚úÖ Simplifies fine-tuning with minimal custom code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('gpt2-custom/tokenizer_config.json',\n",
              " 'gpt2-custom/special_tokens_map.json',\n",
              " 'gpt2-custom/tokenizer.json')"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Save to Hugging Face Hub or local\n",
        "trainer.save_model(\"gpt2-custom\")\n",
        "tokenizer.save_pretrained(\"gpt2-custom\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This cell **saves your fine-tuned GPT-2 model and tokenizer** to a local directory (`gpt2-custom`) so you can reload and use it later.\n",
        "\n",
        "---\n",
        "\n",
        "### üîç Line-by-Line Explanation\n",
        "\n",
        "#### ‚úÖ Save the fine-tuned model\n",
        "\n",
        "```python\n",
        "trainer.save_model(\"gpt2-custom\")\n",
        "```\n",
        "\n",
        "* Saves:\n",
        "\n",
        "  * The **model weights** (`pytorch_model.bin`)\n",
        "  * The **model config** (`config.json`)\n",
        "* Location: local directory named `\"gpt2-custom\"`\n",
        "\n",
        "You can later reload it via:\n",
        "\n",
        "```python\n",
        "from transformers import GPT2LMHeadModel\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2-custom\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### ‚úÖ Save the tokenizer\n",
        "\n",
        "```python\n",
        "tokenizer.save_pretrained(\"gpt2-custom\")\n",
        "```\n",
        "\n",
        "* Saves:\n",
        "\n",
        "  * The tokenizer files (e.g., `tokenizer.json`, `vocab.json`, `merges.txt`)\n",
        "  * A config file (`tokenizer_config.json`)\n",
        "\n",
        "You can reload it using:\n",
        "\n",
        "```python\n",
        "from transformers import GPT2Tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-custom\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Why this matters:\n",
        "\n",
        "Saving both the **model** and **tokenizer** together ensures:\n",
        "\n",
        "* You can **reload** the model later for inference or further training.\n",
        "* Anyone else (or yourself in a future session) can reuse the model without retraining.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'generated_text': \"Once upon a time for a business's support to be applied to the\"}]"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "pipe = pipeline(\"text-generation\", model=\"gpt2-custom\", tokenizer=tokenizer, device=-1)\n",
        "pipe(\"Once upon a time\", max_new_tokens=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "### üß† Why use `pipeline`?\n",
        "\n",
        "* It's **quick** and **convenient** for inference/testing.\n",
        "* Abstracts away the need to manually tokenize and decode text.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Both `max_new_tokens` (=64) and `max_length`(=10) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Once upon a time to be used for the second kilogram of a rate of a rate of 1.0 kg per liter of 1.5 kg of water. This method should be applied to control the seed rate of 3. This method is to be applied as per hectare. Additionally, it is the use of 1.5 kg per hectare\n"
          ]
        }
      ],
      "source": [
        "output = pipe(\n",
        "    \"Once upon a time\",\n",
        "    max_length=10,     # max tokens generated (including prompt length)\n",
        "    temperature=0.7,   # randomness: lower is more conservative, higher more creative\n",
        "    top_p=0.9,         # nucleus sampling, keep tokens with cumulative prob up to 0.9\n",
        "    num_return_sequences=1,  # number of generated sequences\n",
        "    max_new_tokens=64,\n",
        ")\n",
        "\n",
        "print(output[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Gradio web app"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7861\n",
            "* To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=30) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=30) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=30) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=30) and `max_length`(=64) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        }
      ],
      "source": [
        "import gradio as gr\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=\"gpt2-custom\", tokenizer=\"gpt2-custom\", device=-1)\n",
        "\n",
        "def generate_text(prompt, max_length=64, temperature=0.7, top_p=0.9):\n",
        "    outputs = pipe(\n",
        "        prompt,\n",
        "        max_length=max_length,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        num_return_sequences=1,\n",
        "        max_new_tokens=30\n",
        "    )\n",
        "    return outputs[0]['generated_text']\n",
        "\n",
        "# Define Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=generate_text,\n",
        "    inputs=[\n",
        "        gr.Textbox(lines=2, placeholder=\"Enter prompt here...\"),\n",
        "        gr.Slider(10, 200, value=50, label=\"Max Length\"),\n",
        "        gr.Slider(0.1, 1.0, value=0.7, label=\"Temperature\"),\n",
        "        gr.Slider(0.1, 1.0, value=0.9, label=\"Top-p (nucleus sampling)\"),\n",
        "    ],\n",
        "    outputs=\"text\",\n",
        "    title=\"Custom GPT-2 Text Generation\",\n",
        "    description=\"Generate text with your fine-tuned GPT-2 model. Adjust parameters to control creativity.\"\n",
        ")\n",
        "\n",
        "iface.launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "training the model for more epochs will improve the result. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What you‚Äôre seeing is a **classic symptom of a small, undertrained language model** ‚Äî here's why it's happening and what you can do about it.\n",
        "\n",
        "---\n",
        "\n",
        "### üîç Why This Is Happening\n",
        "\n",
        "You're using a **tiny custom GPT-2 model** trained:\n",
        "\n",
        "* From scratch (no pretrained weights),\n",
        "* With a **small number of layers** (4 layers, 256 embedding dim),\n",
        "* For **just 3 epochs**, likely on limited or domain-specific data (e.g., agriculture-related or technical instructions).\n",
        "\n",
        "Because of this:\n",
        "\n",
        "* The model **hasn't learned general language patterns**.\n",
        "* It **memorizes patterns** or phrases from training data (e.g., \"per hectare\", \"kg per liter\", \"seed rate\").\n",
        "* It **regurgitates phrases** even on unrelated prompts due to **overfitting** and **lack of generalization**.\n",
        "\n",
        "So even if you say `\"Once upon a time\"` or `\"Explain quantum physics\"`, it still babbles about kilograms and hectares.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### ‚úÖ How to Fix or Improve It\n",
        "\n",
        "#### **1. Use Pretrained GPT-2 + Fine-tune**\n",
        "\n",
        "Instead of training from scratch, load `gpt2` (124M) from Hugging Face and fine-tune on your data:\n",
        "\n",
        "```python\n",
        "from transformers import GPT2LMHeadModel\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")  # Load pretrained weights\n",
        "model.resize_token_embeddings(len(tokenizer))    # Resize for custom tokenizer\n",
        "```\n",
        "\n",
        "This way, the model starts with **general language understanding** and just learns your domain.\n",
        "\n",
        "---\n",
        "\n",
        "#### **2. Increase Dataset Size + Training Time**\n",
        "\n",
        "If you **must train from scratch**, then:\n",
        "\n",
        "* Increase dataset size (ideally **millions of tokens**)\n",
        "* Train for **at least 10+ epochs**\n",
        "* Use **FP32** (not FP16) unless you're sure the model is stable\n",
        "* Add more randomness or augmentation in the data\n",
        "\n",
        "---\n",
        "\n",
        "#### **3. Prompt the Model Differently**\n",
        "\n",
        "Sometimes, providing a more structured or forced prompt helps:\n",
        "\n",
        "```python\n",
        "pipe(\"Write a short story about a dragon and a mountain. Once upon a time\", max_new_tokens=50)\n",
        "```\n",
        "\n",
        "This forces the model away from its memorized phrasing (though with limited success if it's severely overfit).\n",
        "\n",
        "---\n",
        "\n",
        "#### **4. Inspect Training Data Bias**\n",
        "\n",
        "Check if your training data contains lots of agricultural-style instructional text. That would explain the output you‚Äôre getting. The model is just **repeating what it saw most often**.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† TL;DR\n",
        "\n",
        "* Your model outputs agriculture-like gibberish because it's **small, undertrained, and overfit**.\n",
        "* The best fix is to **start from pretrained GPT-2**, then fine-tune.\n",
        "* Alternatively, collect more **diverse and larger** data, and train for longer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# folder structure explanation for Trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The folder structure you're showing is the **output of a Hugging Face `Trainer` fine-tuning run** for a GPT-2 model. Let me break down the structure and explain what each file or folder does:\n",
        "\n",
        "---\n",
        "\n",
        "## üìÅ `gpt2-custom/`\n",
        "\n",
        "This is your **main output directory**, passed via:\n",
        "\n",
        "```python\n",
        "TrainingArguments(output_dir=\"./gpt2-custom\")\n",
        "```\n",
        "\n",
        "### ‚úÖ Top-level files\n",
        "\n",
        "These represent the **final saved model state** (after training completes or the last checkpoint):\n",
        "\n",
        "| File                                       | Purpose                                                                                            |\n",
        "| ------------------------------------------ | -------------------------------------------------------------------------------------------------- |\n",
        "| `config.json`                              | GPT-2 model architecture (layers, heads, vocab size, etc.).                                        |\n",
        "| `pytorch_model.bin` or `model.safetensors` | Final trained model weights. If you chose `safetensors`, it's saved securely and more efficiently. |\n",
        "| `generation_config.json`                   | Settings for text generation (e.g., temperature, top\\_k, max tokens).                              |\n",
        "| `tokenizer_config.json`                    | Tokenizer configuration (like special tokens).                                                     |\n",
        "| `tokenizer.json`                           | Full serialized tokenizer with merges/vocab.                                                       |\n",
        "| `special_tokens_map.json`                  | Maps token names (`<bos>`, `<eos>`, `<pad>`) to actual IDs.                                        |\n",
        "| `training_args.bin`                        | Your full `TrainingArguments` object saved as a binary.                                            |\n",
        "| `trainer_state.json`                       | Training progress: steps done, logs, metrics.                                                      |\n",
        "\n",
        "---\n",
        "\n",
        "## üìÅ `checkpoint-11040/`\n",
        "\n",
        "This is a **checkpoint saved mid-training** ‚Äî in your case, at step 11,040 ‚Äî because you specified:\n",
        "\n",
        "```python\n",
        "save_steps=50\n",
        "```\n",
        "\n",
        "So every 50 steps, Hugging Face Trainer saves a checkpoint.\n",
        "\n",
        "### ‚úÖ Files in `checkpoint-11040/`\n",
        "\n",
        "| File                                       | Purpose                                                             |\n",
        "| ------------------------------------------ | ------------------------------------------------------------------- |\n",
        "| `config.json`                              | Copy of model architecture at this step.                            |\n",
        "| `model.safetensors` or `pytorch_model.bin` | Model weights at this checkpoint.                                   |\n",
        "| `optimizer.pt`                             | Optimizer state (AdamW, etc.) so training can resume exactly.       |\n",
        "| `scheduler.pt`                             | Learning rate scheduler state.                                      |\n",
        "| `scaler.pt`                                | For mixed-precision (FP16) training ‚Äî stores gradient scaling info. |\n",
        "| `rng_state.pth`                            | RNG seed state (PyTorch, NumPy, Python) for exact reproducibility.  |\n",
        "| `trainer_state.json`                       | Metrics and status of this specific checkpoint.                     |\n",
        "\n",
        "---\n",
        "\n",
        "### üîÅ Why this is useful\n",
        "\n",
        "* You can resume training from `checkpoint-11040/` if interrupted.\n",
        "* You can load a **specific earlier checkpoint** for evaluation or debugging.\n",
        "* The top-level `gpt2-custom/` contains the **final model** unless you stopped mid-run.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Example: Load from a checkpoint\n",
        "\n",
        "```python\n",
        "from transformers import GPT2LMHeadModel\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2-custom/checkpoint-11040\")\n",
        "```\n",
        "\n",
        "Or load from the final trained model:\n",
        "\n",
        "```python\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2-custom\")\n",
        "```\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
