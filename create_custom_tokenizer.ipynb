{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qdBenTpEKSYf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/mushfiq/Desktop/llm_fundamentals/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Fetch the token securely from Colab secrets\n",
        "hf_token = os.getenv(\"HF_TOKEN\")\n",
        "\n",
        "if hf_token is None:\n",
        "    raise ValueError(\"HF_TOKEN not found. Please add it via Colab secrets.\")\n",
        "\n",
        "login(token=hf_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OG_TDiFzRzxE"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import unicodedata\n",
        "\n",
        "def normalize_text(text: str) -> str:\n",
        "    text = unicodedata.normalize(\"NFKC\", text)                # Normalize Unicode\n",
        "    text = text.replace(\"‚Äô\", \"'\")                             # Fix smart apostrophe\n",
        "    text = text.replace(\"‚Äú\", '\"').replace(\"‚Äù\", '\"')           # Fix smart quotes\n",
        "    text = text.replace(\"‚Äì\", \"-\").replace(\"‚Äî\", \"-\")           # Fix dashes\n",
        "    text = re.sub(r\"[^\\x00-\\x7F]+\", \" \", text)                # Remove any remaining non-ASCII\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()                  # Collapse whitespace\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhUWzxqCLHgJ",
        "outputId": "3b3cbb42-d755-48a9-e536-524a451ecb98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Loaded 5572 samples\n",
            "Example: Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def load_corpus(file_path: str) -> list:\n",
        "    df = pd.read_csv(file_path, encoding=\"latin-1\")\n",
        "    text_data = df[\"v2\"].dropna().astype(str).tolist()\n",
        "    text_data = [normalize_text(t) for t in text_data]\n",
        "    return text_data\n",
        "\n",
        "corpus = load_corpus(\"data/spam.csv\")\n",
        "print(f\"‚úÖ Loaded {len(corpus)} samples\")\n",
        "print(f\"Example: {corpus[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "16X50gIJLyfd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from tokenizers import ByteLevelBPETokenizer\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace, ByteLevel\n",
        "import os\n",
        "\n",
        "def train_tokenizer(corpus: list, output_dir: str, vocab_size: int = 10000) -> ByteLevelBPETokenizer:\n",
        "    \"\"\"\n",
        "    Trains a Byte-Level BPE tokenizer on a given corpus.\n",
        "\n",
        "    Args:\n",
        "        corpus (list): List of text strings.\n",
        "        output_dir (str): Path to save tokenizer files.\n",
        "        vocab_size (int): Vocabulary size to train on.\n",
        "\n",
        "    Returns:\n",
        "        ByteLevelBPETokenizer: Trained tokenizer.\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    tokenizer = ByteLevelBPETokenizer()\n",
        "    tokenizer._tokenizer.pre_tokenizer = ByteLevel()\n",
        "\n",
        "\n",
        "    tokenizer.train_from_iterator(\n",
        "        corpus,\n",
        "        vocab_size=vocab_size,\n",
        "        min_frequency=2,\n",
        "        show_progress=True,\n",
        "        special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"]\n",
        "    )\n",
        "    tokenizer.save_model(output_dir)\n",
        "    tokenizer.save(f\"{output_dir}/tokenizer.json\")  # ‚úÖ Required for Hugging Face compatibility\n",
        "    return tokenizer\n",
        "\n",
        "tokenizer = train_tokenizer(corpus, output_dir=\"my_tokenizer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "### üß† Step 1: Initialize the tokenizer\n",
        "\n",
        "```python\n",
        "tokenizer = ByteLevelBPETokenizer()\n",
        "tokenizer._tokenizer.pre_tokenizer = ByteLevel()\n",
        "```\n",
        "\n",
        "* `ByteLevelBPETokenizer()` initializes a tokenizer that:\n",
        "\n",
        "  * Operates at the byte level (good for handling rare or multilingual text).\n",
        "  * Uses Byte Pair Encoding (BPE) to learn subword units.\n",
        "\n",
        "* `tokenizer._tokenizer.pre_tokenizer = ByteLevel()` sets the **pre-tokenizer** to `ByteLevel`, which:\n",
        "\n",
        "  * Encodes the input text into bytes first.\n",
        "  * Handles spaces explicitly by marking them with a special character like `ƒ†` (important for preserving word boundaries).\n",
        "\n",
        "‚ö†Ô∏è Note: This low-level `. _tokenizer.pre_tokenizer` usage is necessary because `ByteLevelBPETokenizer` uses a legacy API; the higher-level abstraction is `tokenizer.pre_tokenizer`.\n",
        "\n",
        "---\n",
        "\n",
        "### üß™ Step 2: Train the tokenizer\n",
        "\n",
        "```python\n",
        "tokenizer.train_from_iterator(\n",
        "    corpus,\n",
        "    vocab_size=vocab_size,\n",
        "    min_frequency=2,\n",
        "    show_progress=True,\n",
        "    special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"]\n",
        ")\n",
        "```\n",
        "\n",
        "This tells the tokenizer to:\n",
        "\n",
        "* Train using the given `corpus` (an iterator of texts).\n",
        "* Limit the vocab size to `vocab_size`.\n",
        "* Ignore tokens that occur less than `min_frequency=2`.\n",
        "* Show progress during training.\n",
        "* Add 5 **special tokens** often used in NLP:\n",
        "\n",
        "  * `<s>` = start of sentence\n",
        "  * `</s>` = end of sentence\n",
        "  * `<pad>` = padding\n",
        "  * `<unk>` = unknown token\n",
        "  * `<mask>` = for masked language modeling\n",
        "\n",
        "---\n",
        "\n",
        "### üíæ Step 3: Save the tokenizer model\n",
        "\n",
        "```python\n",
        "tokenizer.save_model(output_dir)\n",
        "tokenizer.save(f\"{output_dir}/tokenizer.json\")\n",
        "```\n",
        "\n",
        "* `save_model()` saves the `vocab.json` and `merges.txt` files ‚Äî required for legacy loading.\n",
        "* `save(\"tokenizer.json\")` saves a **Hugging Face compatible** single-file JSON format, usable by `PreTrainedTokenizerFast`.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Final Output\n",
        "\n",
        "```python\n",
        "return tokenizer\n",
        "```\n",
        "\n",
        "Returns the trained `ByteLevelBPETokenizer` object so it can be reused if needed.\n",
        "\n",
        "---\n",
        "\n",
        "### üîÑ Summary of Output Files in `output_dir`\n",
        "\n",
        "| File             | Purpose                                |\n",
        "| ---------------- | -------------------------------------- |\n",
        "| `vocab.json`     | Maps tokens to IDs                     |\n",
        "| `merges.txt`     | Contains merge rules learned by BPE    |\n",
        "| `tokenizer.json` | Hugging Face-compatible tokenizer file |\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ae79EKliNcrp"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
          ]
        }
      ],
      "source": [
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "def load_fast_tokenizer(tokenizer_dir: str) -> PreTrainedTokenizerFast:\n",
        "    \"\"\"\n",
        "    Loads tokenizer files into a PreTrainedTokenizerFast object.\n",
        "\n",
        "    Args:\n",
        "        tokenizer_dir (str): Directory where tokenizer files are saved.\n",
        "\n",
        "    Returns:\n",
        "        PreTrainedTokenizerFast: Hugging Face compatible tokenizer.\n",
        "    \"\"\"\n",
        "    return PreTrainedTokenizerFast(\n",
        "        tokenizer_file=f\"{tokenizer_dir}/tokenizer.json\",\n",
        "        unk_token=\"<unk>\",\n",
        "        pad_token=\"<pad>\",\n",
        "        bos_token=\"<s>\",\n",
        "        eos_token=\"</s>\",\n",
        "        mask_token=\"<mask>\"\n",
        "    )\n",
        "\n",
        "fast_tokenizer = load_fast_tokenizer(\"my_tokenizer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `fast_tokenizer` here refers to a **Hugging Face-compatible tokenizer** that is created by wrapping the trained tokenizer files (`tokenizer.json`, `vocab.json`, `merges.txt`) using the class:\n",
        "\n",
        "```python\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üí° What is `PreTrainedTokenizerFast`?\n",
        "\n",
        "`PreTrainedTokenizerFast` is a **fast implementation of Hugging Face tokenizers** using the [ü§ó `tokenizers` Rust-based library](https://github.com/huggingface/tokenizers), which is:\n",
        "\n",
        "* ‚ö° **Much faster** than Python-based tokenizers.\n",
        "* ‚úÖ Fully compatible with `transformers` models.\n",
        "* üß† Capable of handling everything: tokenization, detokenization, ID mapping, padding, truncation, etc.\n",
        "* üß© Works with any tokenizer trained and saved as a `tokenizer.json`.\n",
        "\n",
        "---\n",
        "\n",
        "### üîÑ What the code does\n",
        "\n",
        "```python\n",
        "fast_tokenizer = load_fast_tokenizer(\"my_tokenizer\")\n",
        "```\n",
        "\n",
        "This loads the custom tokenizer you trained earlier with `ByteLevelBPETokenizer` and wraps it into a Hugging Face-compatible format.\n",
        "\n",
        "The important part is here:\n",
        "\n",
        "```python\n",
        "PreTrainedTokenizerFast(\n",
        "    tokenizer_file=f\"{tokenizer_dir}/tokenizer.json\",\n",
        "    unk_token=\"<unk>\",\n",
        "    pad_token=\"<pad>\",\n",
        "    bos_token=\"<s>\",\n",
        "    eos_token=\"</s>\",\n",
        "    mask_token=\"<mask>\"\n",
        ")\n",
        "```\n",
        "\n",
        "This:\n",
        "\n",
        "* Loads `tokenizer.json` (which contains vocab, merges, config).\n",
        "* Defines all special tokens to work seamlessly with Transformer models.\n",
        "\n",
        "Now, you can use `fast_tokenizer` just like any pretrained tokenizer (e.g., `BertTokenizerFast`, `GPT2TokenizerFast`, etc.):\n",
        "\n",
        "```python\n",
        "text = \"Hello world!\"\n",
        "tokens = fast_tokenizer.tokenize(text)\n",
        "ids = fast_tokenizer.convert_tokens_to_ids(tokens)\n",
        "decoded = fast_tokenizer.decode(ids)\n",
        "\n",
        "print(tokens)  # ['H', 'ello', 'ƒ†world', '!']\n",
        "print(ids)     # [id1, id2, id3, id4]\n",
        "print(decoded) # 'Hello world!'\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Summary\n",
        "\n",
        "| Concept                   | What it does                                                               |\n",
        "| ------------------------- | -------------------------------------------------------------------------- |\n",
        "| `PreTrainedTokenizerFast` | A wrapper to load and use a tokenizer trained with `tokenizers`            |\n",
        "| `tokenizer.json`          | The tokenizer model (vocab, merges, config)                                |\n",
        "| `fast_tokenizer`          | The fully functional tokenizer you can now use in your pipeline            |\n",
        "| Benefit                   | Fast, efficient, and works with Hugging Face `transformers` out-of-the-box |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkE_97-KOfNu",
        "outputId": "ef66e480-6684-4797-bd18-d3b948c111e2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No files have been modified since last commit. Skipping to prevent empty commit.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Tokenizer pushed: https://huggingface.co/mushfiqurrobin/spam-tokenizer-bpe\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import HfApi\n",
        "\n",
        "HF_USERNAME = HfApi().whoami()[\"name\"]\n",
        "REPO_NAME = \"spam-tokenizer-bpe\"\n",
        "TOKENIZER_REPO = f\"{HF_USERNAME}/{REPO_NAME}\"\n",
        "\n",
        "fast_tokenizer.save_pretrained(\"my_tokenizer\")  # includes config files\n",
        "fast_tokenizer.push_to_hub(REPO_NAME)\n",
        "\n",
        "print(f\"‚úÖ Tokenizer pushed: https://huggingface.co/{TOKENIZER_REPO}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEqfroiuPRJR",
        "outputId": "946fe34b-0a5a-40e7-ee6c-a0a7dfe1e705"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text: Congratulations! You‚Äôve won a free ticket to Bahamas.\n",
            "Tokens: ['C', 'ongrat', 'ulations', '!', 'ƒ†You', '√¢', 'ƒ¢', 'ƒª', 've', 'ƒ†won', 'ƒ†a', 'ƒ†free', 'ƒ†ticket', 'ƒ†to', 'ƒ†Bahamas', '.']\n",
            "Token IDs: [39, 1322, 2254, 5, 410, 163, 227, 252, 294, 689, 262, 645, 4175, 276, 9676, 18]\n"
          ]
        }
      ],
      "source": [
        "text = \"Congratulations! You‚Äôve won a free ticket to Bahamas.\"\n",
        "tokens = fast_tokenizer.tokenize(text)\n",
        "ids = fast_tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "print(\"Text:\", text)\n",
        "print(\"Tokens:\", tokens)\n",
        "print(\"Token IDs:\", ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAYtiOD0XqhE",
        "outputId": "a9fe015c-0545-4ef5-842e-088657ebadf1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokens: ['H', 'ello', '!', 'ƒ†This', 'ƒ†is', 'ƒ†a', 'ƒ†different', 'ƒ†sentence', 'ƒ†to', 'ƒ†tok', 'en', 'ize', '.']\n",
            "Token IDs: [44, 7284, 5, 839, 327, 262, 2933, 6447, 276, 5917, 307, 2300, 18]\n"
          ]
        }
      ],
      "source": [
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "# Load the tokenizer from the directory\n",
        "fast_tokenizer = PreTrainedTokenizerFast.from_pretrained(\"my_tokenizer\")\n",
        "\n",
        "# New text to tokenize\n",
        "text = \"Hello! This is a different sentence to tokenize.\"\n",
        "\n",
        "# Tokenize and get token IDs\n",
        "tokens = fast_tokenizer.tokenize(text)\n",
        "# print([t.replace(\"ƒ†\", \"\") for t in tokens])\n",
        "token_ids = fast_tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "print(\"Tokens:\", tokens)\n",
        "print(\"Token IDs:\", token_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `ƒ†` character you're seeing in the token strings (e.g., `'ƒ†This'`, `'ƒ†different'`, `'ƒ†sentence'`) is **not an actual character in the final tokenized text**‚Äîit's just a **display artifact** from how some tokenizers (especially Byte-Level BPE tokenizers and GPT-style tokenizers) represent **word boundaries**.\n",
        "\n",
        "### üîç What `ƒ†` Means\n",
        "\n",
        "* `ƒ†` (Unicode U+0120) is used by some tokenizers to indicate that the token starts with a **space**.\n",
        "* It helps distinguish between:\n",
        "\n",
        "  * `\"ƒ†token\"` ‚Üí the word \"token\" preceded by a space.\n",
        "  * `\"token\"` ‚Üí the word \"token\" without a preceding space (e.g., part of a longer word like `\"notoken\"`).\n",
        "\n",
        "### ‚úÖ Why It's Useful\n",
        "\n",
        "This distinction allows the tokenizer to:\n",
        "\n",
        "* **Model spaces explicitly**, which is important for text generation and reconstruction.\n",
        "* **Preserve the structure of the input text** more accurately during training.\n",
        "\n",
        "### üßº If You Want to Remove `ƒ†` from Display\n",
        "\n",
        "Just strip it during display if it's confusing:\n",
        "\n",
        "```python\n",
        "print([t.replace(\"ƒ†\", \"\") for t in tokens])\n",
        "```\n",
        "\n",
        "### ‚ö†Ô∏è But: Don't Remove It Before Encoding\n",
        "\n",
        "The tokenizer uses `ƒ†` internally to tokenize and reconstruct text correctly. You should **not** remove or alter it in any preprocessing unless you're just cleaning up **display/output**.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# creating a tokenizer on a HF dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Creating CSV from Arrow format: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:00<00:00, 47.29ba/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset saved to: data/questions_and_answers.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "import os\n",
        "\n",
        "dataset = load_dataset(\"SoorajK1/questions_and_answers\")\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "dataset[\"train\"].to_csv(\"data/questions_and_answers.csv\", index=False)\n",
        "print(\"Dataset saved to: data/questions_and_answers.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import unicodedata\n",
        "import pandas as pd\n",
        "\n",
        "def normalize_text(text: str) -> str:\n",
        "    text = unicodedata.normalize(\"NFKC\", text)\n",
        "    text = text.replace(\"‚Äô\", \"'\").replace(\"‚Äú\", '\"').replace(\"‚Äù\", '\"')\n",
        "    text = text.replace(\"‚Äì\", \"-\").replace(\"‚Äî\", \"-\")\n",
        "    text = re.sub(r\"[^\\x00-\\x7F]+\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['Unnamed: 0', 'question', 'answer', 'content_row'], dtype='object')"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pd.read_csv(\"data/questions_and_answers.csv\").columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 29438 samples from Q&A dataset\n",
            "Example: What is the purpose of the Android App mentioned? The Android App mentioned in the text is designed to collect data using smartphones. It serves as a tool that allows farmers to input and store information related to their farming activities.\n"
          ]
        }
      ],
      "source": [
        "def load_corpus_from_csv(path: str) -> list:\n",
        "    df = pd.read_csv(path)\n",
        "    # Combine questions and answers\n",
        "    combined_text = (df[\"question\"].astype(str) + \"\\n\" + df[\"answer\"].astype(str)).tolist()\n",
        "    combined_text = [normalize_text(t) for t in combined_text if t.strip()]\n",
        "    return combined_text\n",
        "\n",
        "corpus = load_corpus_from_csv(\"data/questions_and_answers.csv\")\n",
        "print(f\"Loaded {len(corpus)} samples from Q&A dataset\")\n",
        "print(f\"Example: {corpus[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "‚úÖ Tokenizer trained and saved to 'my_qa_tokenizer'\n"
          ]
        }
      ],
      "source": [
        "from tokenizers import ByteLevelBPETokenizer\n",
        "from tokenizers.pre_tokenizers import ByteLevel\n",
        "\n",
        "def train_tokenizer(corpus: list, output_dir: str, vocab_size: int = 20000) -> ByteLevelBPETokenizer:\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    tokenizer = ByteLevelBPETokenizer()\n",
        "    tokenizer._tokenizer.pre_tokenizer = ByteLevel()\n",
        "\n",
        "    tokenizer.train_from_iterator(\n",
        "        corpus,\n",
        "        vocab_size=vocab_size,\n",
        "        min_frequency=2,\n",
        "        show_progress=True,\n",
        "        special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"]\n",
        "    )\n",
        "    tokenizer.save_model(output_dir)\n",
        "    tokenizer.save(f\"{output_dir}/tokenizer.json\")\n",
        "    return tokenizer\n",
        "\n",
        "tokenizer = train_tokenizer(corpus, \"my_qa_tokenizer\")\n",
        "print(\"‚úÖ Tokenizer trained and saved to 'my_qa_tokenizer'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample text: Yes, the Android App can be used offline, without requiring an internet connection for data collection.     Farmers can input data into the app even in remote areas where internet access may be limited.     The data collected offline can be synced and uploaded to the app's server once an internet connection becomes available.\n",
            "Tokens: ['Y', 'es', ',', 'the', 'Android', 'App', 'can', 'be', 'used', 'off', 'line', ',', 'without', 'requiring', 'an', 'internet', 'connection', 'for', 'data', 'collection', '.', '', '', '', '', 'Farmers', 'can', 'input', 'data', 'into', 'the', 'app', 'even', 'in', 'remote', 'areas', 'where', 'internet', 'access', 'may', 'be', 'limited', '.', '', '', '', '', 'The', 'data', 'collected', 'off', 'line', 'can', 'be', 'syn', 'ced', 'and', 'uploaded', 'to', 'the', 'app', \"'s\", 'ser', 'ver', 'once', 'an', 'internet', 'connection', 'becomes', 'available', '.']\n",
            "Token IDs: [61, 275, 16, 266, 8822, 2619, 342, 312, 524, 1078, 3138, 16, 1365, 8573, 286, 13133, 9414, 309, 1537, 4598, 18, 225, 225, 225, 225, 1376, 342, 1492, 1537, 823, 266, 434, 1768, 287, 6746, 1086, 1271, 13133, 1472, 698, 312, 2717, 18, 225, 225, 225, 225, 320, 1537, 3201, 1078, 3138, 342, 312, 10010, 1900, 292, 8484, 295, 266, 434, 821, 3352, 403, 4494, 286, 13133, 9414, 2751, 1316, 18]\n"
          ]
        }
      ],
      "source": [
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "fast_tokenizer = PreTrainedTokenizerFast(\n",
        "    tokenizer_file=\"my_qa_tokenizer/tokenizer.json\",\n",
        "    unk_token=\"<unk>\",\n",
        "    pad_token=\"<pad>\",\n",
        "    bos_token=\"<s>\",\n",
        "    eos_token=\"</s>\",\n",
        "    mask_token=\"<mask>\"\n",
        ")\n",
        "\n",
        "sample_text = \"Yes, the Android App can be used offline, without requiring an internet connection for data collection. \\\n",
        "    Farmers can input data into the app even in remote areas where internet access may be limited. \\\n",
        "    The data collected offline can be synced and uploaded to the app's server once an internet connection becomes available.\"\n",
        "tokens = fast_tokenizer.tokenize(sample_text)\n",
        "ids = fast_tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "print(\"Sample text:\", sample_text)\n",
        "print(\"Tokens:\", [t.replace(\"ƒ†\", \"\") for t in tokens])\n",
        "print(\"Token IDs:\", ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No files have been modified since last commit. Skipping to prevent empty commit.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Tokenizer pushed: https://huggingface.co/mushfiqurrobin/qa-tokenizer-bpe\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import HfApi\n",
        "\n",
        "HF_USERNAME = HfApi().whoami()[\"name\"]\n",
        "REPO_NAME = \"qa-tokenizer-bpe\"\n",
        "TOKENIZER_REPO = f\"{HF_USERNAME}/{REPO_NAME}\"\n",
        "\n",
        "fast_tokenizer.save_pretrained(\"my_qa_tokenizer\")  # includes config files\n",
        "fast_tokenizer.push_to_hub(REPO_NAME)\n",
        "\n",
        "print(f\"‚úÖ Tokenizer pushed: https://huggingface.co/{TOKENIZER_REPO}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokens: ['B', 'uilding', 'a', 'to', 'ken', 'izer', 'from', 'scrat', 'ch', 'might', 'seem', 'd', 'a', 'unting', 'at', 'first', ',', 'but', 'with', 'the', 'right', 'tools', 'and', 'a', 'step', '-', 'by', '-', 'step', 'approach', ',', 'it', \"'s\", 'quite', 'manageable', '.']\n",
            "Token IDs: [38, 10743, 264, 295, 3743, 7545, 453, 14266, 359, 5625, 15894, 294, 69, 12043, 477, 1273, 16, 1418, 417, 266, 2379, 5852, 292, 264, 2011, 17, 3723, 17, 13507, 1926, 16, 376, 821, 12520, 15040, 18]\n"
          ]
        }
      ],
      "source": [
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "# Load the tokenizer from the directory\n",
        "fast_tokenizer = PreTrainedTokenizerFast.from_pretrained(\"my_qa_tokenizer\")\n",
        "\n",
        "text = \"Building a tokenizer from scratch might seem daunting at first, but with the right tools and a step-by-step approach, it's quite manageable.\"\n",
        "\n",
        "tokens = fast_tokenizer.tokenize(text)\n",
        "token_ids = fast_tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "print(\"Tokens:\", [t.replace(\"ƒ†\", \"\") for t in tokens])\n",
        "print(\"Token IDs:\", token_ids)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
