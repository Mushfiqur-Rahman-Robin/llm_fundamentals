{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ZZnsYu0MP_lg",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZnsYu0MP_lg",
        "outputId": "1ca07670-b54e-470d-e32f-30ee65091f3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m108.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision tqdm --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c94fb83c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c94fb83c",
        "outputId": "a01440fd-c196-4ca7-89cc-e5a0e641ba73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch: 2.6.0+cu124\n",
            "CUDA available: True\n"
          ]
        }
      ],
      "source": [
        "import torch, torchvision\n",
        "print(\"PyTorch:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6b47bc93",
      "metadata": {
        "id": "6b47bc93"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "9sc_QW2tQMJU",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9sc_QW2tQMJU",
        "outputId": "0234d95b-38fb-46a3-ba0f-a8eb5e69a7a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a28e4d1d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a28e4d1d",
        "outputId": "8b6fe0fc-698b-441f-d739-45d45db01784"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170M/170M [00:13<00:00, 13.1MB/s]\n"
          ]
        }
      ],
      "source": [
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./cifar_data', train=True, download=True, transform=transform_train)\n",
        "testset = torchvision.datasets.CIFAR10(root='./cifar_data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "trainloader = DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
        "testloader = DataLoader(testset, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56f343be",
      "metadata": {
        "id": "56f343be"
      },
      "source": [
        "# Load VGG-16 as Teacher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "4c892756",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c892756",
        "outputId": "4bd0b81d-2f07-475f-8d09-0f0268ba6431"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
            "100%|██████████| 528M/528M [00:06<00:00, 79.4MB/s]\n"
          ]
        }
      ],
      "source": [
        "teacher_model = torchvision.models.vgg16(pretrained=True)\n",
        "teacher_model.classifier[6] = nn.Linear(4096, 10)\n",
        "teacher_model = teacher_model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b81de9aa",
      "metadata": {
        "id": "b81de9aa"
      },
      "source": [
        "# Define Student Model (3-layer CNN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "8feaae0d",
      "metadata": {
        "id": "8feaae0d"
      },
      "outputs": [],
      "source": [
        "class StudentCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(StudentCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc1 = nn.Linear(64 * 4 * 4, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
        "        x = x.view(-1, 64 * 4 * 4)\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "student_model = StudentCNN().to(device)\n",
        "student_model_hard = StudentCNN().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GQJJPYMrQoT3",
      "metadata": {
        "id": "GQJJPYMrQoT3"
      },
      "source": [
        "# Define Distillation Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "s7h1AztWQnsk",
      "metadata": {
        "id": "s7h1AztWQnsk"
      },
      "outputs": [],
      "source": [
        "def distillation_loss(y_pred, y_teacher, y_true, temperature=2.0, alpha=0.7):\n",
        "    soft_loss = nn.KLDivLoss(reduction='batchmean')(F.log_softmax(y_pred / temperature, dim=1),\n",
        "                                                    F.softmax(y_teacher / temperature, dim=1)) * (temperature ** 2)\n",
        "    hard_loss = nn.CrossEntropyLoss()(y_pred, y_true)\n",
        "    return alpha * soft_loss + (1 - alpha) * hard_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CnluvAPnQ0h7",
      "metadata": {
        "id": "CnluvAPnQ0h7"
      },
      "source": [
        "# Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "Q9YmAxB3Q1DQ",
      "metadata": {
        "id": "Q9YmAxB3Q1DQ"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, delta=0):\n",
        "        self.patience = patience\n",
        "        self.delta = delta\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.counter = 0\n",
        "        self.best_loss = np.inf\n",
        "\n",
        "    def __call__(self, val_loss, model, path):\n",
        "        score = -val_loss\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model, path)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model, path)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model, path):\n",
        "        torch.save(model.state_dict(), path)\n",
        "        self.best_loss = val_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2df70150",
      "metadata": {
        "id": "2df70150"
      },
      "source": [
        "# Evaluate Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "ffbeb1fa",
      "metadata": {
        "id": "ffbeb1fa"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, testloader, criterion, teacher=None):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in testloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            if teacher:\n",
        "                teacher_outputs = teacher(inputs)\n",
        "                loss = criterion(outputs, teacher_outputs, labels)\n",
        "            else:\n",
        "                loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return total_loss / len(testloader), 100 * correct / total"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5eJD4CQkREd5",
      "metadata": {
        "id": "5eJD4CQkREd5"
      },
      "source": [
        "# training function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "w5Omx1AVRHAa",
      "metadata": {
        "id": "w5Omx1AVRHAa"
      },
      "outputs": [],
      "source": [
        "def train_model(model, trainloader, criterion, optimizer, epochs, early_stopping=None, teacher=None, distillation=False, scheduler=None):\n",
        "    model.train()\n",
        "    start_time = time.time()\n",
        "    for epoch in tqdm(range(epochs), desc=\"Epochs\", total=epochs):\n",
        "        running_loss = 0.0\n",
        "        epoch_start_time = time.time()\n",
        "        for inputs, labels in trainloader:  # No inner tqdm to avoid console clutter\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            if distillation and teacher:\n",
        "                with torch.no_grad():\n",
        "                    teacher_outputs = teacher(inputs)\n",
        "                loss = criterion(outputs, teacher_outputs, labels)\n",
        "            else:\n",
        "                loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        avg_loss = running_loss / len(trainloader)\n",
        "        epoch_time = time.time() - epoch_start_time\n",
        "        epochs_remaining = epochs - (epoch + 1)\n",
        "\n",
        "        # Validation\n",
        "        val_loss, val_acc = evaluate_model(model, testloader, criterion, teacher if distillation else None)\n",
        "\n",
        "        # Print epoch-level information to console\n",
        "        print(f'\\nEpoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, Time: {epoch_time:.2f}s')\n",
        "        print(f'Epochs Remaining: {epochs_remaining}, Estimated Time Remaining: {epoch_time * epochs_remaining:.2f}s')\n",
        "        print(f'Validation Loss: {val_loss:.4f}, Accuracy: {val_acc:.4f}')\n",
        "\n",
        "        if early_stopping:\n",
        "            early_stopping(val_loss, model, f'checkpoint_{model.__class__.__name__}.pt')\n",
        "            if early_stopping.early_stop:\n",
        "                print(\"Early stopping triggered\")\n",
        "                break\n",
        "\n",
        "        if scheduler:\n",
        "            scheduler.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5aae591e",
      "metadata": {
        "id": "5aae591e"
      },
      "source": [
        "# Train Teacher Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "a1f94028",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1f94028",
        "outputId": "dbf49de6-f023-499e-dc83-2aa75fa38a24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Teacher Model (VGG-16)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpochs:   0%|          | 0/20 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/20, Loss: 0.9545, Time: 52.10s\n",
            "Epochs Remaining: 19, Estimated Time Remaining: 989.87s\n",
            "Validation Loss: 0.6208, Accuracy: 78.9100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpochs:   5%|▌         | 1/20 [00:58<18:26, 58.21s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 2/20, Loss: 0.6132, Time: 52.21s\n",
            "Epochs Remaining: 18, Estimated Time Remaining: 939.84s\n",
            "Validation Loss: 0.4944, Accuracy: 83.1600\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpochs:  10%|█         | 2/20 [01:57<17:39, 58.87s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 3/20, Loss: 0.5148, Time: 53.35s\n",
            "Epochs Remaining: 17, Estimated Time Remaining: 906.97s\n",
            "Validation Loss: 0.4655, Accuracy: 84.0700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpochs:  15%|█▌        | 3/20 [02:57<16:46, 59.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 4/20, Loss: 0.4632, Time: 53.39s\n",
            "Epochs Remaining: 16, Estimated Time Remaining: 854.24s\n",
            "Validation Loss: 0.4121, Accuracy: 85.9800\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpochs:  20%|██        | 4/20 [03:58<16:00, 60.04s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 5/20, Loss: 0.4195, Time: 53.24s\n",
            "Epochs Remaining: 15, Estimated Time Remaining: 798.60s\n",
            "Validation Loss: 0.3831, Accuracy: 86.8100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpochs:  25%|██▌       | 5/20 [04:57<14:57, 59.82s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 6/20, Loss: 0.3367, Time: 53.16s\n",
            "Epochs Remaining: 14, Estimated Time Remaining: 744.21s\n",
            "Validation Loss: 0.3643, Accuracy: 87.8400\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpochs:  30%|███       | 6/20 [05:57<13:55, 59.65s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 7/20, Loss: 0.3199, Time: 53.10s\n",
            "Epochs Remaining: 13, Estimated Time Remaining: 690.28s\n",
            "Validation Loss: 0.3576, Accuracy: 88.0700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpochs:  35%|███▌      | 7/20 [06:58<13:02, 60.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 8/20, Loss: 0.3157, Time: 53.26s\n",
            "Epochs Remaining: 12, Estimated Time Remaining: 639.14s\n",
            "Validation Loss: 0.3556, Accuracy: 88.1700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpochs:  40%|████      | 8/20 [07:56<11:55, 59.60s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 9/20, Loss: 0.3084, Time: 53.34s\n",
            "Epochs Remaining: 11, Estimated Time Remaining: 586.77s\n",
            "Validation Loss: 0.3541, Accuracy: 88.3800\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs:  50%|█████     | 10/20 [09:55<09:52, 59.22s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 10/20, Loss: 0.3031, Time: 53.49s\n",
            "Epochs Remaining: 10, Estimated Time Remaining: 534.90s\n",
            "Validation Loss: 0.3553, Accuracy: 88.1900\n",
            "\n",
            "Epoch 11/20, Loss: 0.2965, Time: 52.98s\n",
            "Epochs Remaining: 9, Estimated Time Remaining: 476.83s\n",
            "Validation Loss: 0.3526, Accuracy: 88.2700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpochs:  55%|█████▌    | 11/20 [10:56<08:58, 59.80s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 12/20, Loss: 0.2947, Time: 53.33s\n",
            "Epochs Remaining: 8, Estimated Time Remaining: 426.66s\n",
            "Validation Loss: 0.3519, Accuracy: 88.3200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs:  65%|██████▌   | 13/20 [12:54<06:54, 59.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 13/20, Loss: 0.2880, Time: 53.48s\n",
            "Epochs Remaining: 7, Estimated Time Remaining: 374.34s\n",
            "Validation Loss: 0.3527, Accuracy: 88.3000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpochs:  70%|███████   | 14/20 [13:50<05:50, 58.34s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 14/20, Loss: 0.2898, Time: 53.01s\n",
            "Epochs Remaining: 6, Estimated Time Remaining: 318.05s\n",
            "Validation Loss: 0.3530, Accuracy: 88.3600\n",
            "\n",
            "Epoch 15/20, Loss: 0.2896, Time: 53.58s\n",
            "Epochs Remaining: 5, Estimated Time Remaining: 267.88s\n",
            "Validation Loss: 0.3513, Accuracy: 88.4000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs:  80%|████████  | 16/20 [15:49<03:54, 58.63s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 16/20, Loss: 0.2899, Time: 53.24s\n",
            "Epochs Remaining: 4, Estimated Time Remaining: 212.95s\n",
            "Validation Loss: 0.3515, Accuracy: 88.4400\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpochs:  85%|████████▌ | 17/20 [16:45<02:53, 57.96s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 17/20, Loss: 0.2890, Time: 53.12s\n",
            "Epochs Remaining: 3, Estimated Time Remaining: 159.37s\n",
            "Validation Loss: 0.3516, Accuracy: 88.4500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpochs:  85%|████████▌ | 17/20 [17:42<03:07, 62.51s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 18/20, Loss: 0.2890, Time: 53.00s\n",
            "Epochs Remaining: 2, Estimated Time Remaining: 105.99s\n",
            "Validation Loss: 0.3517, Accuracy: 88.4800\n",
            "Early stopping triggered\n",
            "\n",
            "Evaluating Teacher Model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Teacher Test Loss: 0.3517, Accuracy: 88.4800%\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nTraining Teacher Model (VGG-16)...\")\n",
        "teacher_optimizer = optim.SGD([\n",
        "    {'params': teacher_model.features.parameters(), 'lr': 1e-3},\n",
        "    {'params': teacher_model.classifier.parameters(), 'lr': 1e-2}\n",
        "], momentum=0.9, weight_decay=5e-4)\n",
        "scheduler = optim.lr_scheduler.StepLR(teacher_optimizer, step_size=5, gamma=0.1)\n",
        "teacher_criterion = nn.CrossEntropyLoss()\n",
        "early_stopping_teacher = EarlyStopping(patience=3)\n",
        "train_model(teacher_model, trainloader, teacher_criterion, teacher_optimizer, epochs=20,\n",
        "            early_stopping=early_stopping_teacher, scheduler=scheduler)\n",
        "\n",
        "print(\"\\nEvaluating Teacher Model...\")\n",
        "teacher_loss, teacher_acc = evaluate_model(teacher_model, testloader, teacher_criterion)\n",
        "print(f'Teacher Test Loss: {teacher_loss:.4f}, Accuracy: {teacher_acc:.4f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "6kVitcdKR-lH",
      "metadata": {
        "id": "6kVitcdKR-lH"
      },
      "outputs": [],
      "source": [
        "# Save Teacher Model\n",
        "torch.save(teacher_model.state_dict(), './models/teacher_vgg16.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZnbKYQ_ARn2Q",
      "metadata": {
        "id": "ZnbKYQ_ARn2Q"
      },
      "source": [
        "# Train Student Model with Distillation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "qEw4InkaRoTo",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEw4InkaRoTo",
        "outputId": "0269e173-d606-4606-c6ba-0c156c3c9d0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Student Model with Distillation...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs:   5%|▌         | 1/20 [00:32<10:11, 32.21s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/20, Loss: 3.0998, Time: 28.74s\n",
            "Epochs Remaining: 19, Estimated Time Remaining: 546.09s\n",
            "Validation Loss: 2.5606, Accuracy: 52.9600\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpochs:  10%|█         | 2/20 [01:03<09:27, 31.54s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 2/20, Loss: 2.0821, Time: 27.36s\n",
            "Epochs Remaining: 18, Estimated Time Remaining: 492.43s\n",
            "Validation Loss: 1.8899, Accuracy: 63.2100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpochs:  15%|█▌        | 3/20 [01:37<09:17, 32.80s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 3/20, Loss: 1.7334, Time: 30.56s\n",
            "Epochs Remaining: 17, Estimated Time Remaining: 519.52s\n",
            "Validation Loss: 1.5666, Accuracy: 68.2300\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpochs:  20%|██        | 4/20 [02:07<08:26, 31.66s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 4/20, Loss: 1.5458, Time: 26.53s\n",
            "Epochs Remaining: 16, Estimated Time Remaining: 424.46s\n",
            "Validation Loss: 1.5142, Accuracy: 69.0200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpochs:  25%|██▌       | 5/20 [02:37<07:47, 31.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 5/20, Loss: 1.4189, Time: 26.88s\n",
            "Epochs Remaining: 15, Estimated Time Remaining: 403.27s\n",
            "Validation Loss: 1.3425, Accuracy: 71.9700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpochs:  30%|███       | 6/20 [03:07<07:10, 30.73s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 6/20, Loss: 1.2222, Time: 25.95s\n",
            "Epochs Remaining: 14, Estimated Time Remaining: 363.30s\n",
            "Validation Loss: 1.2211, Accuracy: 73.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpochs:  35%|███▌      | 7/20 [03:37<06:35, 30.40s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 7/20, Loss: 1.1919, Time: 26.36s\n",
            "Epochs Remaining: 13, Estimated Time Remaining: 342.66s\n",
            "Validation Loss: 1.1857, Accuracy: 73.6700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpochs:  40%|████      | 8/20 [04:07<06:02, 30.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 8/20, Loss: 1.1709, Time: 26.37s\n",
            "Epochs Remaining: 12, Estimated Time Remaining: 316.46s\n",
            "Validation Loss: 1.1803, Accuracy: 74.2700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpochs:  45%|████▌     | 9/20 [04:37<05:32, 30.27s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 9/20, Loss: 1.1505, Time: 26.27s\n",
            "Epochs Remaining: 11, Estimated Time Remaining: 289.01s\n",
            "Validation Loss: 1.1545, Accuracy: 74.4400\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpochs:  50%|█████     | 10/20 [05:08<05:04, 30.46s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 10/20, Loss: 1.1345, Time: 27.51s\n",
            "Epochs Remaining: 10, Estimated Time Remaining: 275.13s\n",
            "Validation Loss: 1.1561, Accuracy: 74.1700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpochs:  55%|█████▌    | 11/20 [05:38<04:32, 30.25s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 11/20, Loss: 1.1169, Time: 26.45s\n",
            "Epochs Remaining: 9, Estimated Time Remaining: 238.07s\n",
            "Validation Loss: 1.1440, Accuracy: 74.5200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpochs:  60%|██████    | 12/20 [06:08<04:02, 30.29s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 12/20, Loss: 1.1139, Time: 26.45s\n",
            "Epochs Remaining: 8, Estimated Time Remaining: 211.59s\n",
            "Validation Loss: 1.1361, Accuracy: 74.6700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpochs:  65%|██████▌   | 13/20 [06:38<03:31, 30.15s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 13/20, Loss: 1.1119, Time: 26.43s\n",
            "Epochs Remaining: 7, Estimated Time Remaining: 184.98s\n",
            "Validation Loss: 1.1365, Accuracy: 74.6500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpochs:  70%|███████   | 14/20 [07:08<03:00, 30.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 14/20, Loss: 1.1160, Time: 26.48s\n",
            "Epochs Remaining: 6, Estimated Time Remaining: 158.91s\n",
            "Validation Loss: 1.1343, Accuracy: 74.7000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpochs:  75%|███████▌  | 15/20 [07:38<02:31, 30.22s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 15/20, Loss: 1.1095, Time: 26.96s\n",
            "Epochs Remaining: 5, Estimated Time Remaining: 134.78s\n",
            "Validation Loss: 1.1335, Accuracy: 74.7500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpochs:  80%|████████  | 16/20 [08:08<02:00, 30.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 16/20, Loss: 1.1057, Time: 26.58s\n",
            "Epochs Remaining: 4, Estimated Time Remaining: 106.34s\n",
            "Validation Loss: 1.1323, Accuracy: 74.7200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpochs:  85%|████████▌ | 17/20 [08:40<01:31, 30.51s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 17/20, Loss: 1.1084, Time: 27.05s\n",
            "Epochs Remaining: 3, Estimated Time Remaining: 81.14s\n",
            "Validation Loss: 1.1323, Accuracy: 74.7400\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpochs:  90%|█████████ | 18/20 [09:10<01:00, 30.45s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 18/20, Loss: 1.1015, Time: 26.89s\n",
            "Epochs Remaining: 2, Estimated Time Remaining: 53.77s\n",
            "Validation Loss: 1.1320, Accuracy: 74.7400\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpochs:  95%|█████████▌| 19/20 [09:41<00:30, 30.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 19/20, Loss: 1.1027, Time: 27.49s\n",
            "Epochs Remaining: 1, Estimated Time Remaining: 27.49s\n",
            "Validation Loss: 1.1314, Accuracy: 74.6900\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs: 100%|██████████| 20/20 [10:11<00:00, 30.59s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 20/20, Loss: 1.1041, Time: 26.49s\n",
            "Epochs Remaining: 0, Estimated Time Remaining: 0.00s\n",
            "Validation Loss: 1.1313, Accuracy: 74.7800\n",
            "\n",
            "Evaluating Student Model (Distilled)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Student (Distilled) Test Loss: 1.1313, Accuracy: 74.7800%\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nTraining Student Model with Distillation...\")\n",
        "student_optimizer = optim.Adam(student_model.parameters(), lr=0.001)\n",
        "scheduler = StepLR(student_optimizer, step_size=5, gamma=0.1)\n",
        "early_stopping_student = EarlyStopping(patience=3)\n",
        "train_model(student_model, trainloader, distillation_loss, student_optimizer, epochs=20,\n",
        "            early_stopping=early_stopping_student, teacher=teacher_model, distillation=True, scheduler=scheduler)\n",
        "\n",
        "print(\"\\nEvaluating Student Model (Distilled)...\")\n",
        "student_loss, student_acc = evaluate_model(student_model, testloader, distillation_loss, teacher=teacher_model)\n",
        "print(f'Student (Distilled) Test Loss: {student_loss:.4f}, Accuracy: {student_acc:.4f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "VPKhYnr1SAWE",
      "metadata": {
        "id": "VPKhYnr1SAWE"
      },
      "outputs": [],
      "source": [
        "# Save Student Model\n",
        "torch.save(student_model.state_dict(), './models/student_distilled.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "C_6XcxKqRyrk",
      "metadata": {
        "id": "C_6XcxKqRyrk"
      },
      "source": [
        "# Train Student Model with Hard Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "xzk8MFNkR03K",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzk8MFNkR03K",
        "outputId": "c11f6eb0-e713-41bf-ab4e-e035af034b2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Student Model with Hard Labels...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs:   5%|▌         | 1/20 [00:25<08:06, 25.63s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/20, Loss: 1.6993, Time: 23.27s\n",
            "Epochs Remaining: 19, Estimated Time Remaining: 442.15s\n",
            "Validation Loss: 1.3716, Accuracy: 49.9500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpochs:  10%|█         | 2/20 [00:51<07:39, 25.54s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 2/20, Loss: 1.3033, Time: 23.05s\n",
            "Epochs Remaining: 18, Estimated Time Remaining: 414.92s\n",
            "Validation Loss: 1.1767, Accuracy: 58.4500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpochs:  15%|█▌        | 3/20 [01:16<07:13, 25.48s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 3/20, Loss: 1.1308, Time: 22.99s\n",
            "Epochs Remaining: 17, Estimated Time Remaining: 390.88s\n",
            "Validation Loss: 0.9926, Accuracy: 64.9300\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpochs:  20%|██        | 4/20 [01:42<06:47, 25.48s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 4/20, Loss: 1.0281, Time: 23.08s\n",
            "Epochs Remaining: 16, Estimated Time Remaining: 369.36s\n",
            "Validation Loss: 0.9652, Accuracy: 65.6800\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpochs:  25%|██▌       | 5/20 [02:07<06:22, 25.53s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 5/20, Loss: 0.9717, Time: 23.18s\n",
            "Epochs Remaining: 15, Estimated Time Remaining: 347.75s\n",
            "Validation Loss: 0.8951, Accuracy: 68.4600\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpochs:  30%|███       | 6/20 [02:32<05:56, 25.46s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 6/20, Loss: 0.8587, Time: 22.92s\n",
            "Epochs Remaining: 14, Estimated Time Remaining: 320.91s\n",
            "Validation Loss: 0.7954, Accuracy: 72.2100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpochs:  35%|███▌      | 7/20 [02:58<05:30, 25.42s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 7/20, Loss: 0.8383, Time: 22.95s\n",
            "Epochs Remaining: 13, Estimated Time Remaining: 298.41s\n",
            "Validation Loss: 0.7855, Accuracy: 72.6400\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpochs:  40%|████      | 8/20 [03:23<05:04, 25.41s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 8/20, Loss: 0.8280, Time: 23.02s\n",
            "Epochs Remaining: 12, Estimated Time Remaining: 276.22s\n",
            "Validation Loss: 0.7851, Accuracy: 72.9300\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpochs:  45%|████▌     | 9/20 [03:49<04:39, 25.45s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 9/20, Loss: 0.8210, Time: 23.16s\n",
            "Epochs Remaining: 11, Estimated Time Remaining: 254.73s\n",
            "Validation Loss: 0.7709, Accuracy: 73.3800\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpochs:  50%|█████     | 10/20 [04:15<04:17, 25.75s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 10/20, Loss: 0.8057, Time: 23.99s\n",
            "Epochs Remaining: 10, Estimated Time Remaining: 239.91s\n",
            "Validation Loss: 0.7727, Accuracy: 73.4800\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpochs:  55%|█████▌    | 11/20 [04:40<03:50, 25.59s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 11/20, Loss: 0.7963, Time: 22.85s\n",
            "Epochs Remaining: 9, Estimated Time Remaining: 205.68s\n",
            "Validation Loss: 0.7627, Accuracy: 73.7800\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpochs:  60%|██████    | 12/20 [05:06<03:24, 25.51s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 12/20, Loss: 0.7960, Time: 23.00s\n",
            "Epochs Remaining: 8, Estimated Time Remaining: 183.98s\n",
            "Validation Loss: 0.7582, Accuracy: 73.9200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpochs:  65%|██████▌   | 13/20 [05:31<02:58, 25.44s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 13/20, Loss: 0.7919, Time: 22.92s\n",
            "Epochs Remaining: 7, Estimated Time Remaining: 160.47s\n",
            "Validation Loss: 0.7620, Accuracy: 73.7500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpochs:  70%|███████   | 14/20 [05:56<02:32, 25.35s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 14/20, Loss: 0.7895, Time: 22.79s\n",
            "Epochs Remaining: 6, Estimated Time Remaining: 136.73s\n",
            "Validation Loss: 0.7595, Accuracy: 73.7800\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpochs:  75%|███████▌  | 15/20 [06:21<02:06, 25.25s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 15/20, Loss: 0.7944, Time: 22.68s\n",
            "Epochs Remaining: 5, Estimated Time Remaining: 113.41s\n",
            "Validation Loss: 0.7563, Accuracy: 73.9400\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpochs:  80%|████████  | 16/20 [06:46<01:40, 25.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 16/20, Loss: 0.7879, Time: 22.55s\n",
            "Epochs Remaining: 4, Estimated Time Remaining: 90.19s\n",
            "Validation Loss: 0.7567, Accuracy: 73.9300\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpochs:  85%|████████▌ | 17/20 [07:11<01:15, 25.14s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 17/20, Loss: 0.7891, Time: 22.51s\n",
            "Epochs Remaining: 3, Estimated Time Remaining: 67.54s\n",
            "Validation Loss: 0.7566, Accuracy: 73.9300\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpochs:  85%|████████▌ | 17/20 [07:36<01:20, 26.88s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 18/20, Loss: 0.7860, Time: 22.48s\n",
            "Epochs Remaining: 2, Estimated Time Remaining: 44.95s\n",
            "Validation Loss: 0.7565, Accuracy: 73.9500\n",
            "Early stopping triggered\n",
            "\n",
            "Evaluating Student Model (Hard Labels)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Student (Hard Labels) Test Loss: 0.7565, Accuracy: 73.9500%\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nTraining Student Model with Hard Labels...\")\n",
        "student_hard_optimizer = optim.Adam(student_model_hard.parameters(), lr=0.001)\n",
        "scheduler = StepLR(student_hard_optimizer, step_size=5, gamma=0.1)\n",
        "student_hard_criterion = nn.CrossEntropyLoss()\n",
        "early_stopping_student_hard = EarlyStopping(patience=3)\n",
        "train_model(student_model_hard, trainloader, student_hard_criterion, student_hard_optimizer, epochs=20,\n",
        "            early_stopping=early_stopping_student_hard, scheduler=scheduler)\n",
        "\n",
        "print(\"\\nEvaluating Student Model (Hard Labels)...\")\n",
        "student_hard_loss, student_hard_acc = evaluate_model(student_model_hard, testloader, student_hard_criterion)\n",
        "print(f'Student (Hard Labels) Test Loss: {student_hard_loss:.4f}, Accuracy: {student_hard_acc:.4f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "fpoozVdRSCi5",
      "metadata": {
        "id": "fpoozVdRSCi5"
      },
      "outputs": [],
      "source": [
        "# Save Student Model (Hard Labels)\n",
        "torch.save(student_model_hard.state_dict(), './models/student_hard.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4FTvY6gaSJqr",
      "metadata": {
        "id": "4FTvY6gaSJqr"
      },
      "source": [
        "# Inference on a Single Image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Zw7Lhlr6ph07",
      "metadata": {
        "id": "Zw7Lhlr6ph07"
      },
      "source": [
        "## randomly select 10 images from cifar-10 test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "6c9LS4I0polZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c9LS4I0polZ",
        "outputId": "b4af74ab-2d27-411a-de24-9f924881e3f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded test_batch: 10000 samples\n",
            "Image tensor shape: torch.Size([10000, 3, 32, 32])\n",
            "Label tensor shape: torch.Size([10000])\n",
            "First 5 labels: [3, 8, 8, 0, 6]\n",
            "\n",
            "Selected 10 random images:\n",
            "Index: 6768, Class: bird, Saved as: ./images/cifar10_bird_6768.png\n",
            "Index: 3333, Class: bird, Saved as: ./images/cifar10_bird_3333.png\n",
            "Index: 4045, Class: horse, Saved as: ./images/cifar10_horse_4045.png\n",
            "Index: 8301, Class: dog, Saved as: ./images/cifar10_dog_8301.png\n",
            "Index: 6845, Class: bird, Saved as: ./images/cifar10_bird_6845.png\n",
            "Index: 3617, Class: cat, Saved as: ./images/cifar10_cat_3617.png\n",
            "Index: 2002, Class: deer, Saved as: ./images/cifar10_deer_2002.png\n",
            "Index: 955, Class: horse, Saved as: ./images/cifar10_horse_955.png\n",
            "Index: 959, Class: ship, Saved as: ./images/cifar10_ship_959.png\n",
            "Index: 2555, Class: horse, Saved as: ./images/cifar10_horse_2555.png\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "def unpickle(file):\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "    return dict\n",
        "\n",
        "test_batch_path = '/content/cifar_data/cifar-10-batches-py/test_batch'\n",
        "\n",
        "# Load test_batch data\n",
        "test_data_dict = unpickle(test_batch_path)\n",
        "\n",
        "# Extract images and labels\n",
        "images = test_data_dict[b'data']  # Shape: (10000, 3072)\n",
        "labels = test_data_dict[b'labels']  # Shape: (10000,)\n",
        "\n",
        "# Reshape images to (10000, 3, 32, 32)\n",
        "images = images.reshape(-1, 3, 32, 32).astype(np.float32) / 255.0  # Normalize to [0, 1]\n",
        "\n",
        "num_samples = 10\n",
        "random_indices = np.random.choice(len(images), size=num_samples, replace=False)\n",
        "selected_images = images[random_indices]  # Shape: (10, 3, 32, 32)\n",
        "selected_labels = np.array(labels)[random_indices]\n",
        "\n",
        "os.makedirs('images', exist_ok=True)\n",
        "classes = ('airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "# Save selected images as PNG files\n",
        "for i, (img, label_idx) in enumerate(zip(selected_images, selected_labels)):\n",
        "    # Transpose image to (32, 32, 3) for PIL\n",
        "    img = img.transpose(1, 2, 0)  # Shape: (32, 32, 3)\n",
        "    # Convert to uint8 for saving\n",
        "    img = (img * 255).astype(np.uint8)\n",
        "    # Create PIL image\n",
        "    pil_img = Image.fromarray(img)\n",
        "    # Save with descriptive filename\n",
        "    class_name = classes[label_idx]\n",
        "    filename = f'./images/cifar10_{class_name}_{random_indices[i]}.png'\n",
        "    pil_img.save(filename)\n",
        "\n",
        "# Convert all images and labels to PyTorch tensors\n",
        "images_tensor = torch.tensor(images, dtype=torch.float32)\n",
        "labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "# Apply VGG-16 normalization\n",
        "transform = transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
        "# Apply normalization to each image\n",
        "for i in range(images_tensor.size(0)):\n",
        "    images_tensor[i] = transform(images_tensor[i])\n",
        "\n",
        "test_dataset = TensorDataset(images_tensor, labels_tensor)\n",
        "testloader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\"Loaded test_batch: {len(test_dataset)} samples\")\n",
        "print(f\"Image tensor shape: {images_tensor.shape}\")\n",
        "print(f\"Label tensor shape: {labels_tensor.shape}\")\n",
        "print(f\"First 5 labels: {labels_tensor[:5].tolist()}\")\n",
        "print(\"\\nSelected 10 random images:\")\n",
        "for idx, label_idx in zip(random_indices, selected_labels):\n",
        "    print(f\"Index: {idx}, Class: {classes[label_idx]}, Saved as: ./images/cifar10_{classes[label_idx]}_{idx}.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07f607cd",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "### 🔹 `# Load test_batch data`\n",
        "\n",
        "```python\n",
        "test_data_dict = unpickle(test_batch_path)\n",
        "```\n",
        "\n",
        "**Why:**\n",
        "You are loading the CIFAR-10 test data from a binary `.pkl` file using a custom `unpickle` function. The CIFAR-10 dataset is split into multiple batches stored as Python pickled dictionaries.\n",
        "\n",
        "**Under the hood:**\n",
        "\n",
        "* `pickle.load()` deserializes the byte stream and reconstructs the Python dictionary.\n",
        "* The keys are in byte-string format (e.g., `b'data'`, `b'labels'`).\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 `# Extract images and labels`\n",
        "\n",
        "```python\n",
        "images = test_data_dict[b'data']  # Shape: (10000, 3072)\n",
        "labels = test_data_dict[b'labels']  # Shape: (10000,)\n",
        "```\n",
        "\n",
        "**Why:**\n",
        "You access the actual image and label data from the dictionary.\n",
        "\n",
        "**Under the hood:**\n",
        "\n",
        "* `images`: 2D NumPy array where each row is a flattened 32×32 RGB image → 32×32×3 = **3072 features per image**.\n",
        "* `labels`: A list of integers (0–9) representing class IDs.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 `# Reshape images to (10000, 3, 32, 32)`\n",
        "\n",
        "```python\n",
        "images = images.reshape(-1, 3, 32, 32).astype(np.float32) / 255.0\n",
        "```\n",
        "\n",
        "**Why:**\n",
        "\n",
        "* Neural networks expect image input in `(C, H, W)` format (channel-first), not flattened vectors.\n",
        "* Normalization (dividing by 255.0) scales the image values to `[0.0, 1.0]`.\n",
        "\n",
        "**Under the hood:**\n",
        "\n",
        "* `reshape(-1, 3, 32, 32)`: Takes 3072 values per row and arranges them into a 3D tensor.\n",
        "\n",
        "  * The original format is: **\\[R(1024) | G(1024) | B(1024)]**.\n",
        "  * So reshape will correctly map the first 1024 to Red channel, next to Green, etc.\n",
        "* `astype(np.float32)`: Ensures compatibility with PyTorch tensors (float32 is expected).\n",
        "* `/ 255.0`: Converts pixel range from `[0, 255]` to `[0, 1]`.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 `# Save selected images as PNG files`\n",
        "\n",
        "```python\n",
        "img = img.transpose(1, 2, 0)  # Shape: (32, 32, 3)\n",
        "img = (img * 255).astype(np.uint8)\n",
        "```\n",
        "\n",
        "**Why:**\n",
        "\n",
        "* PIL expects images in `(H, W, C)` format and in `uint8` type for saving or displaying.\n",
        "\n",
        "**Under the hood:**\n",
        "\n",
        "* `transpose(1, 2, 0)`: Swaps from (C, H, W) to (H, W, C).\n",
        "* `(img * 255).astype(np.uint8)`: Converts normalized float image `[0,1]` back to integers `[0,255]`.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 `# Convert all images and labels to PyTorch tensors`\n",
        "\n",
        "```python\n",
        "images_tensor = torch.tensor(images, dtype=torch.float32)\n",
        "labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
        "```\n",
        "\n",
        "**Why:**\n",
        "You need PyTorch tensors for training/inference with neural networks.\n",
        "\n",
        "**Under the hood:**\n",
        "\n",
        "* `torch.tensor(...)`: Converts NumPy arrays to GPU/CPU-compatible tensors.\n",
        "* `dtype=torch.float32` is required for image input to models.\n",
        "* `dtype=torch.long` is required for classification targets (used in `nn.CrossEntropyLoss`).\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 `# Apply VGG-16 normalization`\n",
        "\n",
        "```python\n",
        "transform = transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
        "```\n",
        "\n",
        "**Why:**\n",
        "Pretrained models like **VGG-16** expect input normalized to **ImageNet statistics**. This ensures the image distribution matches what the model saw during training.\n",
        "\n",
        "**Under the hood:**\n",
        "\n",
        "* `transforms.Normalize()` subtracts mean and divides by std **per channel**.\n",
        "* It doesn't apply anything yet — it's just a callable object stored in `transform`.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 `# Apply normalization to each image`\n",
        "\n",
        "```python\n",
        "for i in range(images_tensor.size(0)):\n",
        "    images_tensor[i] = transform(images_tensor[i])\n",
        "```\n",
        "\n",
        "**Why:**\n",
        "You manually apply the normalization transform to each image.\n",
        "\n",
        "**Under the hood:**\n",
        "\n",
        "* `images_tensor[i]` is a single 3x32x32 tensor.\n",
        "* `transform(...)` subtracts mean and divides std for each channel.\n",
        "* This is done manually instead of using a `torchvision.transforms.Compose()` pipeline.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 `# Create TensorDataset and DataLoader`\n",
        "\n",
        "```python\n",
        "test_dataset = TensorDataset(images_tensor, labels_tensor)\n",
        "testloader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "```\n",
        "\n",
        "**Why:**\n",
        "You package the tensors into a dataset and set up a `DataLoader` to handle batching and loading.\n",
        "\n",
        "**Under the hood:**\n",
        "\n",
        "* `TensorDataset`: pairs up image and label tensors.\n",
        "* `DataLoader`: fetches batches of size 32, optionally in parallel using 2 workers.\n",
        "\n",
        "  * `shuffle=False`: ensures consistent order (important during evaluation).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "AZOhEzPNSKOT",
      "metadata": {
        "id": "AZOhEzPNSKOT"
      },
      "outputs": [],
      "source": [
        "def infer_single_image(model, image_path, transform, model_name):\n",
        "    model.eval()\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image = transform(image).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        output = model(image)\n",
        "        _, predicted = torch.max(output, 1)\n",
        "    return classes[predicted.item()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WP2Oebs7SSdd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "id": "WP2Oebs7SSdd",
        "outputId": "c8a56c40-10db-41b7-ccf1-081e2a9952d3"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Teacher Model Prediction: bird\n",
            "Student (Distilled) Prediction: bird\n",
            "Student (Hard Labels) Prediction: bird\n",
            "\n",
            "Teacher (VGG-16) Accuracy: 88.48%\n",
            "Student (Distilled) Accuracy: 74.78%\n",
            "Student (Hard Labels) Accuracy: 73.95%\n"
          ]
        }
      ],
      "source": [
        "image_path = '/content/images/cifar10_bird_3333.png'\n",
        "image = Image.open(image_path).convert('RGB')\n",
        "image_np = np.array(image)\n",
        "\n",
        "plt.figure(figsize=(4, 4))\n",
        "plt.imshow(image_np)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Inference with Teacher\n",
        "teacher_model.load_state_dict(torch.load('./models/teacher_vgg16.pt'))\n",
        "teacher_prediction = infer_single_image(teacher_model, image_path, transform_test, 'Teacher')\n",
        "print(f\"Teacher Model Prediction: {teacher_prediction}\")\n",
        "\n",
        "# Inference with Student (Distilled)\n",
        "student_model.load_state_dict(torch.load('./models/student_distilled.pt'))\n",
        "student_prediction = infer_single_image(student_model, image_path, transform_test, 'Student (Distilled)')\n",
        "print(f\"Student (Distilled) Prediction: {student_prediction}\")\n",
        "\n",
        "# Inference with Student (Hard Labels)\n",
        "student_model_hard.load_state_dict(torch.load('./models/student_hard.pt'))\n",
        "student_hard_prediction = infer_single_image(student_model_hard, image_path, transform_test, 'Student (Hard Labels)')\n",
        "print(f\"Student (Hard Labels) Prediction: {student_hard_prediction}\")\n",
        "\n",
        "print(f\"\\nTeacher (VGG-16) Accuracy: {teacher_acc:.2f}%\")\n",
        "print(f\"Student (Distilled) Accuracy: {student_acc:.2f}%\")\n",
        "print(f\"Student (Hard Labels) Accuracy: {student_hard_acc:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
